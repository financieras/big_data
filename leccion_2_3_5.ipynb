{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIDFGFJBPgyV5y3KLgMqJv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/financieras/big_data/blob/main/leccion_2_3_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lección 2.3.5: Integración de BI con pipelines de datos\n",
        "\n",
        "## 1. ¿Por qué integrar BI con pipelines de datos?\n",
        "\n",
        "Los datos son como el combustible de un coche: sin un suministro constante, limpio y confiable, incluso el mejor dashboard de Business Intelligence (BI) se queda parado. La **integración de BI con pipelines de datos** asegura que los datos lleguen frescos, estructurados y listos para ser analizados, transformando reportes estáticos en **herramientas dinámicas para decisiones en tiempo real**.\n",
        "\n",
        "**Beneficios clave:**\n",
        "- **Actualización automática:** Dashboards siempre reflejan la realidad.\n",
        "- **Consistencia:** Evita métricas contradictorias entre equipos.\n",
        "- **Escalabilidad:** Soporta desde startups hasta petabytes de datos.\n",
        "- **Eficiencia:** Reduce el tiempo manual de analistas.\n",
        "\n",
        "> **Analogía:** Un pipeline es como la cocina de un restaurante: prepara los ingredientes (datos) para que el chef (BI) cree platos deliciosos (dashboards).\n",
        "\n",
        "**Problema sin integración:**  \n",
        "Datos obsoletos, procesos manuales y errores humanos. Ejemplo: un retailer que actualiza su inventario semanalmente pierde ventas por falta de stock.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Recordatorio: Pipelines y BI\n",
        "\n",
        "- **Pipeline de datos:** Secuencia automatizada que **extrae** datos (de APIs, bases de datos), los **transforma** (limpieza, agregación) y los **carga** en un destino (data warehouse). Puede ser **ETL** (transformación antes de cargar) o **ELT** (transformación en la nube).\n",
        "- **BI (Business Intelligence):** Herramientas como Power BI, Tableau o Looker Studio que visualizan datos para responder preguntas de negocio. Sin pipelines, dependen de procesos manuales.\n",
        "\n",
        "> **Ejemplo:** En una app de delivery, un pipeline extrae datos de pedidos en tiempo real, calcula métricas (tiempo promedio de entrega) y las carga en BigQuery. El dashboard de Looker Studio muestra estas métricas al equipo de operaciones cada 10 minutos.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Arquitectura de integración: Cómo conectar los puntos\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[Fuentes: CRM, APIs, Bases de datos] --> B(Pipeline ETL/ELT)\n",
        "    B --> |Transformación| C[Data Warehouse: BigQuery, Snowflake]\n",
        "    C --> D[BI: Power BI, Tableau, Looker Studio]\n",
        "    D --> E[Usuarios: Analistas, Gerentes, CEOs]\n",
        "```\n",
        "\n",
        "**Componentes clave:**\n",
        "1. **Fuentes:** APIs (Salesforce), bases de datos (PostgreSQL), archivos (CSV, JSON).\n",
        "2. **Pipeline:** Herramientas como Airflow (orquestación), dbt (transformación SQL) o Spark (procesamiento masivo).\n",
        "3. **Almacenamiento:** Data warehouses (Snowflake, BigQuery) o data lakes (AWS S3).\n",
        "4. **BI:** Conexión directa via ODBC/JDBC o APIs nativas.\n",
        "\n",
        "> **Tip:** Usa un **data warehouse** como punto central para garantizar datos limpios y optimizados para BI.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Ejemplo práctico: Retail en tiempo real\n",
        "\n",
        "**Contexto:**  \n",
        "\"ModaRápida\", una cadena de 30 tiendas, necesita un dashboard para monitorear ventas e inventario en tiempo real.\n",
        "\n",
        "**Pipeline implementado:**\n",
        "```\n",
        "[Ventas en tiendas] → [API REST] → [Kafka para streaming] → [Spark para transformación] → [BigQuery] → [Power BI]\n",
        "[Inventario RFID] → [API inventario] →\n",
        "```\n",
        "\n",
        "**Código ilustrativo (Python + Spark):**\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Iniciar Spark\n",
        "spark = SparkSession.builder.appName(\"RetailPipeline\").getOrCreate()\n",
        "\n",
        "# Extraer datos de Kafka\n",
        "ventas = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"ventas\").load()\n",
        "\n",
        "# Transformar: calcular ventas por región\n",
        "ventas_por_region = ventas.groupBy(\"region\").agg({\"monto\": \"sum\"})\n",
        "\n",
        "# Cargar en BigQuery\n",
        "ventas_por_region.writeStream.format(\"bigquery\").option(\"table\", \"modarapida.ventas\").start()\n",
        "```\n",
        "\n",
        "**Dashboard en Power BI:**\n",
        "- Gráfico de barras: Ventas por región.\n",
        "- Tabla: Stock crítico por tienda.\n",
        "- Actualización: Cada 15 minutos.\n",
        "\n",
        "**Impacto:**\n",
        "| Métrica | Antes | Después |\n",
        "|---------|-------|---------|\n",
        "| Tiempo de actualización | 1 día | 15 minutos |\n",
        "| Ventas perdidas por stock | 10% | 2% |\n",
        "| Decisiones diarias | 3 | 12 |\n",
        "\n",
        "**Resultado:** ModaRápida ajusta inventario en tiempo real, aumentando ventas un 15%.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Herramientas para la integración\n",
        "\n",
        "| Herramienta | Rol | Conector BI | Ventaja | Caso de uso |\n",
        "|-------------|-----|-------------|---------|-------------|\n",
        "| **Apache Airflow** | Orquestación | Indirecto (via warehouse) | Programación flexible | Automatizar ETL nocturno |\n",
        "| **dbt** | Transformación | SQL a BI | Modelado sencillo | Estandarizar métricas |\n",
        "| **Snowflake** | Data Warehouse | ODBC/JDBC | Escalabilidad cloud | Análisis masivo |\n",
        "| **Power BI** | BI | Nativo a warehouses | Integración Microsoft | Reportes corporativos |\n",
        "| **Looker Studio** | BI | Google Cloud | Gratuito, fácil | Startups y PYMEs |\n",
        "\n",
        "> **Recomendación:** Para empezar, prueba **dbt + BigQuery + Looker Studio**. Es accesible y potente.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Comparación: Batch vs. Streaming\n",
        "\n",
        "| Aspecto | Batch (Lotes) | Streaming (Tiempo real) |\n",
        "|---------|---------------|-------------------------|\n",
        "| **Frecuencia** | Diaria, horaria | Segundos, minutos |\n",
        "| **Ejemplo** | Reporte de ventas nocturno | Monitoreo de fraudes |\n",
        "| **Herramientas** | Airflow, dbt | Kafka, Spark Streaming |\n",
        "| **Ventaja** | Simple, económico | Reacción inmediata |\n",
        "| **Desafío** | Retraso en datos | Complejidad técnica |\n",
        "\n",
        "**Ejemplo de código (Batch con Airflow):**\n",
        "```python\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from datetime import datetime\n",
        "\n",
        "def extract_sales():\n",
        "    # Extraer datos\n",
        "    pass\n",
        "\n",
        "def transform_sales():\n",
        "    # Transformar con Pandas/dbt\n",
        "    pass\n",
        "\n",
        "def load_to_bigquery():\n",
        "    # Cargar a BI\n",
        "    pass\n",
        "\n",
        "dag = DAG('ventas_diarias', start_date=datetime(2025, 1, 1), schedule_interval='@daily')\n",
        "extract = PythonOperator(task_id='extract', python_callable=extract_sales, dag=dag)\n",
        "transform = PythonOperator(task_id='transform', python_callable=transform_sales, dag=dag)\n",
        "load = PythonOperator(task_id='load', python_callable=load_to_bigquery, dag=dag)\n",
        "\n",
        "extract >> transform >> load\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Mejores prácticas\n",
        "\n",
        "1. **Colabora entre equipos:** Ingenieros y analistas deben definir métricas juntos.\n",
        "2. **Centraliza transformaciones:** Usa dbt o SQL para evitar lógica duplicada.\n",
        "3. **Monitorea fallos:** Configura alertas con herramientas como Datadog o Prefect.\n",
        "4. **Documenta:** Usa catálogos (DataHub) para rastrear el linaje de datos.\n",
        "5. **Optimiza performance:** Precalcula agregaciones en el warehouse para acelerar BI.\n",
        "\n",
        "> **Tip práctico:** Empieza con un pipeline pequeño (CSV → Google Sheets → Looker Studio) antes de escalar a stacks complejos.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Errores comunes y soluciones\n",
        "\n",
        "| Error | Impacto | Solución |\n",
        "|-------|---------|----------|\n",
        "| Pipeline sin monitoreo | Dashboards obsoletos | Alertas automáticas |\n",
        "| Métricas inconsistentes | Decisiones erróneas | Catálogo de datos centralizado |\n",
        "| Conexión directa a fuentes | BI lento, inestable | Usar data warehouse |\n",
        "| Falta de pruebas | Datos erróneos | Validaciones con Great Expectations |\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Tendencias emergentes\n",
        "\n",
        "- **Reverse ETL:** Llevar datos de BI a sistemas operativos (ej. enviar alertas a Slack con Census).\n",
        "- **Headless BI:** APIs como Cube.js para integrar BI sin interfaces gráficas.\n",
        "- **Augmented Analytics:** Herramientas como ThoughtSpot que usan IA para generar insights automáticamente.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Actividad para alumnos\n",
        "\n",
        "**Mini-proyecto:**\n",
        "1. Descarga un dataset público (ej. ventas en Kaggle).\n",
        "2. Crea un pipeline simple con Python/Pandas que limpie y agregue datos.\n",
        "3. Carga el resultado en Google Sheets o BigQuery (free tier).\n",
        "4. Conecta a Looker Studio y crea un dashboard con 2 visualizaciones.\n",
        "5. Reflexiona: ¿Qué pasa si cambias la estructura de los datos? ¿Cómo lo detectas?\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Resumen\n",
        "\n",
        "- **Pipelines:** Automatizan el flujo de datos desde fuentes hasta destinos.\n",
        "- **BI:** Visualiza datos para decisiones rápidas.\n",
        "- **Integración:** Conecta ambos para dashboards frescos y confiables.\n",
        "- **Herramientas clave:** Airflow, dbt, BigQuery, Power BI.\n",
        "- **Impacto:** Reduce el tiempo entre datos y decisiones, impulsando el negocio.\n",
        "\n",
        "> **Frase inspiradora:** “Un dashboard sin pipeline es como un cuadro sin pintura: solo un marco vacío.”\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Referencias\n",
        "\n",
        "### Vídeos\n",
        "- [Building Modern Data Pipelines for BI](https://youtu.be/Jw_0Zrz0YqM)\n",
        "- [Power BI and Azure Data Factory Integration](https://youtu.be/0pK_6zL2YfU)\n",
        "- [dbt + BI: A Match Made in Heaven](https://youtu.be/abc123)\n",
        "\n",
        "### Lecturas\n",
        "- [Data Pipelines Pocket Reference](https://www.oreilly.com/library/view/data-pipelines-pocket/9781492087823/)\n",
        "- [Modern Data Stack Guide](https://www.moderndatastack.xyz/)\n",
        "- [dbt Best Practices](https://docs.getdbt.com/guides/best-practices)"
      ],
      "metadata": {
        "id": "zbABMKmxIer0"
      }
    }
  ]
}