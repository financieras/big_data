{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO01r9UMiYD4v7mh0+F0PZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/financieras/big_data/blob/main/leccion_2_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lección 2.1.2: Manipulación de datos con Pandas (lectura, limpieza, profiling)\n",
        "\n",
        "## 1. Pandas: El bisturí del data scientist\n",
        "\n",
        "Pandas es para los datos lo que un bisturí para un cirujano: **la herramienta precisa que transforma lo complejo en manejable**. No es exageración—el 80% del trabajo en datos pasa por manipular tablas, y Pandas domina este arte.\n",
        "\n",
        "> **Idea clave:** Pandas convierte datos caóticos en información estructurada lista para análisis.\n",
        "\n",
        "**¿Por qué Pandas es indispensable?**\n",
        "- 📊 Maneja datos tabulares de manera intuitiva (DataFrames)\n",
        "- ⚡ Operaciones vectorizadas: rápidas como C, simples como Python\n",
        "- 🔧 Integración perfecta con el ecosistema de datos\n",
        "- 🤝 Comunidad masiva y documentación excelente\n",
        "\n",
        "**Ejemplo visual de transformación:**\n",
        "\n",
        "**Antes: Datos desordenados en CSV**\n",
        "```\n",
        "nombre,edad,ciudad,salario\n",
        "Ana,28,Madrid,45000\n",
        "Juan,32,Barcelona,52000\n",
        "María,,Valencia,38000\n",
        "Luis,29,Madrid,41000\n",
        "```\n",
        "\n",
        "**Después: DataFrame limpio y estructurado**\n",
        "| nombre | edad | ciudad    | salario |\n",
        "|--------|------|-----------|---------|\n",
        "| Ana    | 28   | Madrid    | 45000   |\n",
        "| Juan   | 32   | Barcelona | 52000   |\n",
        "| María  | 29*  | Valencia  | 38000   |\n",
        "| Luis   | 29   | Madrid    | 41000   |\n",
        "\n",
        "*Nulo imputado con mediana\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Lectura de datos: Tu punto de partida\n",
        "\n",
        "### **Formatos esenciales y cómo leerlos**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# CSV (el caballo de batalla)\n",
        "df = pd.read_csv('datos.csv', encoding='utf-8')\n",
        "\n",
        "# Excel (el favorito del negocio)\n",
        "df = pd.read_excel('reporte.xlsx', sheet_name='Ventas')\n",
        "\n",
        "# JSON (APIs y web)\n",
        "df = pd.read_json('datos_api.json')\n",
        "\n",
        "# Desde SQL\n",
        "import sqlite3\n",
        "conn = sqlite3.connect('database.db')\n",
        "df = pd.read_sql('SELECT * FROM ventas', conn)\n",
        "\n",
        "# Desde URL directa\n",
        "df = pd.read_csv('https://ejemplo.com/datos.csv')\n",
        "\n",
        "# Desde el portapapeles (¡súper útil!)\n",
        "df = pd.read_clipboard()\n",
        "```\n",
        "\n",
        "### **Parámetros que salvan vidas**\n",
        "\n",
        "```python\n",
        "df = pd.read_csv('datos_sucios.csv',\n",
        "                 encoding='latin-1',       # Para caracteres especiales (ñ, á)\n",
        "                 sep=';',                  # Separador diferente a coma\n",
        "                 decimal=',',              # Decimales europeos\n",
        "                 na_values=['NULL', 'N/A', ''],  # Valores faltantes personalizados\n",
        "                 dtype={'telefono': str},  # Forzar tipo de datos\n",
        "                 parse_dates=['fecha'],    # Convertir a fecha automáticamente\n",
        "                 nrows=1000)              # Solo primeras 1000 filas (testing)\n",
        "```\n",
        "\n",
        "**Caso real crítico:** Un analista recibe datos de ventas europeos con decimales con coma. Sin `decimal=','`, el precio **1.200,50€** se interpreta como **1.20050€**. ¡Error catastrófico que puede costar millones!\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Exploración inicial: Los primeros 5 minutos\n",
        "\n",
        "**Estos son los comandos que debes ejecutar SIEMPRE:**\n",
        "\n",
        "```python\n",
        "# Los 5 comandos esenciales\n",
        "print(df.shape)          # Dimensiones (filas, columnas)\n",
        "print(df.info())         # Tipos de datos, memoria, nulos\n",
        "print(df.head())         # Primeras 5 filas\n",
        "print(df.describe())     # Estadísticas de columnas numéricas\n",
        "print(df.columns.tolist())  # Lista de nombres de columnas\n",
        "```\n",
        "\n",
        "### **Inspección avanzada**\n",
        "\n",
        "```python\n",
        "# Muestra aleatoria (mejor que head() para datasets grandes)\n",
        "df.sample(10)\n",
        "\n",
        "# Valores únicos por columna\n",
        "df.nunique()\n",
        "\n",
        "# Verificar duplicados\n",
        "print(f\"Duplicados: {df.duplicated().sum()}\")\n",
        "\n",
        "# Memoria utilizada\n",
        "print(f\"Memoria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Limpieza de datos: De sucio a brillante\n",
        "\n",
        "### **Detección y manejo de valores faltantes**\n",
        "\n",
        "```python\n",
        "# Diagnóstico completo\n",
        "print(\"Valores nulos por columna:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\nPorcentaje de nulos:\")\n",
        "print((df.isnull().mean() * 100).round(2))\n",
        "\n",
        "# Estrategias según el contexto\n",
        "# 1. Eliminar (cuando son pocos y aleatorios)\n",
        "df_clean = df.dropna(subset=['columna_critica'])\n",
        "\n",
        "# 2. Imputar con media/mediana (variables numéricas)\n",
        "df['edad'] = df['edad'].fillna(df['edad'].median())\n",
        "\n",
        "# 3. Imputar con moda (variables categóricas)\n",
        "df['ciudad'] = df['ciudad'].fillna(df['ciudad'].mode()[0])\n",
        "\n",
        "# 4. Imputar por grupos (más inteligente)\n",
        "df['precio'] = df.groupby('categoria')['precio'].transform(\n",
        "    lambda x: x.fillna(x.median())\n",
        ")\n",
        "\n",
        "# 5. Forward fill (datos temporales)\n",
        "df['ventas'] = df['ventas'].fillna(method='ffill')\n",
        "```\n",
        "\n",
        "### **Corrección de tipos de datos**\n",
        "\n",
        "```python\n",
        "# Los tipos incorrectos son bombas de tiempo\n",
        "# Conversiones esenciales\n",
        "df['fecha'] = pd.to_datetime(df['fecha'], errors='coerce')\n",
        "df['precio'] = pd.to_numeric(df['precio'], errors='coerce')\n",
        "df['categoria'] = df['categoria'].astype('category')  # Ahorra memoria\n",
        "\n",
        "# Verificación\n",
        "print(\"Tipos después de la corrección:\")\n",
        "print(df.dtypes)\n",
        "```\n",
        "\n",
        "### **Limpieza de texto y estandarización**\n",
        "\n",
        "```python\n",
        "# Texto inconsistente es un dolor de cabeza común\n",
        "df['nombre'] = df['nombre'].str.strip().str.title()\n",
        "df['email'] = df['email'].str.lower()\n",
        "df['telefono'] = df['telefono'].str.replace(r'[\\s-]', '', regex=True)\n",
        "\n",
        "# Estandarizar categorías\n",
        "mapeo_ciudades = {'MAD': 'Madrid', 'mad': 'Madrid', 'MADRID': 'Madrid'}\n",
        "df['ciudad'] = df['ciudad'].map(mapeo_ciudades).fillna(df['ciudad'])\n",
        "```\n",
        "\n",
        "### **Eliminación de duplicados**\n",
        "\n",
        "```python\n",
        "# Duplicados exactos\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Duplicados en columnas clave\n",
        "df = df.drop_duplicates(subset=['id_usuario', 'fecha'])\n",
        "\n",
        "# Mantener el último registro\n",
        "df = df.drop_duplicates(subset=['id'], keep='last')\n",
        "\n",
        "# Ver duplicados antes de eliminar\n",
        "duplicados = df[df.duplicated(keep=False)]\n",
        "print(f\"Encontrados {len(duplicados)} registros duplicados\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Transformaciones esenciales\n",
        "\n",
        "### **Selección y filtrado**\n",
        "\n",
        "```python\n",
        "# Seleccionar columnas\n",
        "df[['producto', 'precio']]\n",
        "\n",
        "# Filtrar filas\n",
        "df[df['precio'] > 100]\n",
        "df[(df['precio'] > 50) & (df['rating'] >= 4.0)]  # Múltiples condiciones\n",
        "\n",
        "# Query SQL-like (más legible)\n",
        "df.query('precio > 100 and categoria == \"Ropa\"')\n",
        "```\n",
        "\n",
        "### **Creación de nuevas columnas**\n",
        "\n",
        "```python\n",
        "# Columna simple\n",
        "df['precio_total'] = df['precio'] * df['cantidad']\n",
        "\n",
        "# Columna condicional con np.where\n",
        "import numpy as np\n",
        "df['descuento'] = np.where(df['precio'] > 100, df['precio'] * 0.10, 0)\n",
        "\n",
        "# Múltiples condiciones con np.select\n",
        "condiciones = [\n",
        "    (df['precio'] < 50),\n",
        "    (df['precio'] >= 50) & (df['precio'] < 100),\n",
        "    (df['precio'] >= 100)\n",
        "]\n",
        "categorias = ['Básico', 'Medio', 'Premium']\n",
        "df['segmento'] = np.select(condiciones, categorias)\n",
        "```\n",
        "\n",
        "### **Agregaciones y agrupaciones**\n",
        "\n",
        "```python\n",
        "# Agrupar y agregar\n",
        "df.groupby('categoria')['precio'].mean()\n",
        "\n",
        "# Múltiples agregaciones\n",
        "df.groupby('categoria').agg({\n",
        "    'precio': ['mean', 'min', 'max'],\n",
        "    'cantidad': 'sum'\n",
        "})\n",
        "\n",
        "# Pivot tables\n",
        "df.pivot_table(values='precio',\n",
        "               index='categoria',\n",
        "               columns='ciudad',\n",
        "               aggfunc='mean')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Profiling: Conoce a tus datos en profundidad\n",
        "\n",
        "### **Función de análisis de distribución**\n",
        "\n",
        "```python\n",
        "def analizar_distribucion(df, columna):\n",
        "    \"\"\"Análisis estadístico completo de una variable\"\"\"\n",
        "    stats = {\n",
        "        'media': df[columna].mean(),\n",
        "        'mediana': df[columna].median(),\n",
        "        'moda': df[columna].mode()[0] if not df[columna].mode().empty else None,\n",
        "        'std': df[columna].std(),\n",
        "        'q1': df[columna].quantile(0.25),\n",
        "        'q3': df[columna].quantile(0.75),\n",
        "        'iqr': df[columna].quantile(0.75) - df[columna].quantile(0.25),\n",
        "        'skewness': df[columna].skew(),\n",
        "        'kurtosis': df[columna].kurtosis()\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "# Uso\n",
        "stats_ventas = analizar_distribucion(df, 'ventas')\n",
        "print(stats_ventas)\n",
        "```\n",
        "\n",
        "### **Función de reporte de calidad**\n",
        "\n",
        "```python\n",
        "def reporte_calidad(df):\n",
        "    \"\"\"Reporte completo de calidad de datos\"\"\"\n",
        "    calidad = pd.DataFrame({\n",
        "        'tipo_dato': df.dtypes,\n",
        "        'valores_no_nulos': df.count(),\n",
        "        'valores_nulos': df.isnull().sum(),\n",
        "        'porcentaje_nulos': (df.isnull().mean() * 100).round(2),\n",
        "        'valores_unicos': df.nunique(),\n",
        "        'memoria_mb': (df.memory_usage(deep=True) / 1024**2).round(2)\n",
        "    })\n",
        "    return calidad\n",
        "\n",
        "# Reporte ejecutivo\n",
        "print(reporte_calidad(df))\n",
        "```\n",
        "\n",
        "### **Profiling automatizado (opcional)**\n",
        "\n",
        "```python\n",
        "# Pandas Profiling - Reporte HTML completo\n",
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "profile = ProfileReport(df, title=\"Análisis de Ventas\")\n",
        "profile.to_file(\"reporte_eda.html\")\n",
        "\n",
        "# Incluye: estadísticas, distribuciones, correlaciones,\n",
        "# valores faltantes, duplicados, alertas de calidad\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Caso práctico completo: E-commerce\n",
        "\n",
        "**Problema:** Dataset de 10,000 pedidos con múltiples problemas de calidad.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. LECTURA con parámetros específicos\n",
        "pedidos = pd.read_csv('pedidos_ecommerce.csv',\n",
        "                     sep=';',\n",
        "                     decimal=',',\n",
        "                     parse_dates=['fecha_pedido', 'fecha_envio'],\n",
        "                     encoding='latin-1')\n",
        "\n",
        "# 2. DIAGNÓSTICO INICIAL\n",
        "print(\"=== DIAGNÓSTICO INICIAL ===\")\n",
        "print(f\"Shape: {pedidos.shape}\")\n",
        "print(f\"Memoria: {pedidos.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "print(\"\\nValores nulos:\")\n",
        "print(pedidos.isnull().sum())\n",
        "print(f\"\\nDuplicados: {pedidos.duplicated().sum()}\")\n",
        "\n",
        "# 3. LIMPIEZA SISTEMÁTICA\n",
        "\n",
        "# Corrección de precios negativos o cero\n",
        "pedidos = pedidos[pedidos['precio'] > 0]\n",
        "\n",
        "# Imputación de categorías faltantes\n",
        "pedidos['categoria'] = pedidos['categoria'].fillna('OTROS')\n",
        "\n",
        "# Estandarización de ciudades\n",
        "pedidos['ciudad'] = pedidos['ciudad'].str.upper().str.strip()\n",
        "\n",
        "# Eliminación de duplicados\n",
        "pedidos = pedidos.drop_duplicates(subset=['id_pedido'])\n",
        "\n",
        "# Corrección de tipos\n",
        "pedidos['telefono'] = pedidos['telefono'].astype(str)\n",
        "\n",
        "# 4. FEATURE ENGINEERING BÁSICO\n",
        "pedidos['dias_entrega'] = (\n",
        "    pedidos['fecha_envio'] - pedidos['fecha_pedido']\n",
        ").dt.days\n",
        "pedidos['valor_total'] = pedidos['precio'] * pedidos['cantidad']\n",
        "pedidos['mes'] = pedidos['fecha_pedido'].dt.month\n",
        "pedidos['trimestre'] = pedidos['fecha_pedido'].dt.quarter\n",
        "\n",
        "# 5. VALIDACIÓN\n",
        "assert pedidos['precio'].min() > 0, \"Hay precios negativos\"\n",
        "assert pedidos['dias_entrega'].min() >= 0, \"Fechas inconsistentes\"\n",
        "\n",
        "# 6. PROFILING FINAL\n",
        "print(\"\\n=== RESULTADO FINAL ===\")\n",
        "print(f\"Shape final: {pedidos.shape}\")\n",
        "print(reporte_calidad(pedidos))\n",
        "\n",
        "# 7. GUARDAR\n",
        "pedidos.to_csv('pedidos_limpio.csv', index=False)\n",
        "pedidos.to_parquet('pedidos_limpio.parquet')  # Más eficiente\n",
        "```\n",
        "\n",
        "**Resultado cuantificado:**\n",
        "- **Antes:** 10,000 filas, 15% de nulos, tipos incorrectos, 150 duplicados\n",
        "- **Después:** 9,850 filas limpias, 0% nulos en campos críticos, 0 duplicados\n",
        "- **Tiempo de limpieza:** 10 minutos vs 2+ horas manual\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Funciones mágicas para el día a día\n",
        "\n",
        "### **Función de limpieza express**\n",
        "\n",
        "```python\n",
        "def limpieza_express(df):\n",
        "    \"\"\"Limpieza rápida para datasets comunes\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # Eliminar duplicados\n",
        "    df_clean = df_clean.drop_duplicates()\n",
        "    \n",
        "    # Limpiar columnas de texto\n",
        "    text_cols = df_clean.select_dtypes(include=['object']).columns\n",
        "    for col in text_cols:\n",
        "        df_clean[col] = df_clean[col].str.strip().str.lower()\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# Uso rápido\n",
        "df_limpio = limpieza_express(df_original)\n",
        "```\n",
        "\n",
        "### **Pipeline completo de preparación**\n",
        "\n",
        "```python\n",
        "def preparar_para_eda(archivo):\n",
        "    \"\"\"Pipeline completo: datos listos para EDA\"\"\"\n",
        "    # 1. Lectura\n",
        "    df = pd.read_csv(archivo)\n",
        "    \n",
        "    # 2. Limpieza básica\n",
        "    df = limpieza_express(df)\n",
        "    \n",
        "    # 3. Profiling inicial\n",
        "    reporte = reporte_calidad(df)\n",
        "    \n",
        "    return df, reporte\n",
        "\n",
        "# Uso en proyecto real\n",
        "datos_limpios, diagnostico = preparar_para_eda('datos_brutos.csv')\n",
        "print(diagnostico)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Buenas prácticas y errores comunes\n",
        "\n",
        "### ✅ **Haz esto:**\n",
        "\n",
        "```python\n",
        "# Trabajar con copias para no modificar datos originales\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Documentar cada transformación\n",
        "# NOTA: Imputamos nulos en edad con mediana (distribución sesgada)\n",
        "df['edad'] = df['edad'].fillna(df['edad'].median())\n",
        "\n",
        "# Validar resultados con assertions\n",
        "assert df['precio'].min() >= 0, \"ERROR: Hay precios negativos\"\n",
        "assert df.isnull().sum().sum() == 0, \"ERROR: Quedan valores nulos\"\n",
        "\n",
        "# Verificar después de operaciones críticas\n",
        "print(f\"Filas antes: {len(df_original)}\")\n",
        "df_clean = df_original.dropna()\n",
        "print(f\"Filas después: {len(df_clean)}\")\n",
        "```\n",
        "\n",
        "### ❌ **Evita esto:**\n",
        "\n",
        "```python\n",
        "# ❌ Modificar el DataFrame original directamente\n",
        "df = df.dropna()  # ¡Peligro! Pierdes datos originales\n",
        "\n",
        "# ❌ Usar bucles cuando hay vectorización\n",
        "for i in range(len(df)):\n",
        "    df.loc[i, 'nueva'] = df.loc[i, 'col1'] * 2  # LENTO\n",
        "\n",
        "# ✅ Usa operaciones vectorizadas\n",
        "df['nueva'] = df['col1'] * 2  # RÁPIDO\n",
        "\n",
        "# ❌ Ignorar los warnings\n",
        "# Investiga siempre SettingWithCopyWarning y FutureWarning\n",
        "```\n",
        "\n",
        "### **Checklist de manipulación**\n",
        "\n",
        "- [ ] Datos leídos correctamente (encoding, separadores, tipos)\n",
        "- [ ] Exploración inicial completada (`info()`, `describe()`)\n",
        "- [ ] Valores nulos identificados y tratados\n",
        "- [ ] Duplicados eliminados o justificados\n",
        "- [ ] Texto estandarizado y limpio\n",
        "- [ ] Tipos de datos correctos por columna\n",
        "- [ ] Outliers identificados y validados\n",
        "- [ ] Transformaciones documentadas con comentarios\n",
        "- [ ] Validaciones con assertions ejecutadas\n",
        "- [ ] Dataset guardado en formato eficiente\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Resumen\n",
        "\n",
        "**Pandas es:**\n",
        "- ✅ El **estándar** para manipulación de datos en Python\n",
        "- ✅ **Versátil:** Lee cualquier formato (CSV, Excel, JSON, SQL)\n",
        "- ✅ **Potente:** Limpieza, transformación, agregación\n",
        "- ✅ **Rápido:** Operaciones vectorizadas\n",
        "\n",
        "**Flujo típico de trabajo:**\n",
        "```\n",
        "1. Leer → read_csv(), read_excel()\n",
        "2. Explorar → info(), describe(), head()\n",
        "3. Limpiar → Nulos, duplicados, tipos\n",
        "4. Transformar → Filtrar, agrupar, nuevas columnas\n",
        "5. Validar → Assertions, profiling\n",
        "6. Guardar → to_csv(), to_parquet()\n",
        "```\n",
        "\n",
        "**Las 3 operaciones más usadas:**\n",
        "1. `df[df['columna'] > valor]` → Filtrar\n",
        "2. `df.groupby('categoria')['valor'].mean()` → Agrupar\n",
        "3. `df['nueva'] = df['col1'] * df['col2']` → Crear columnas\n",
        "\n",
        "> **Conclusión:** Dominar Pandas es dominar el 80% del trabajo diario de un Data Analyst. Invierte tiempo en aprenderlo bien—cada minuto pagará dividendos exponenciales.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Referencias\n",
        "\n",
        "### Vídeos\n",
        "- [Complete Pandas Tutorial](https://youtu.be/vmEHCJofslg) - Tutorial completo\n",
        "- [Data Cleaning with Pandas](https://youtu.be/example2) - Limpieza práctica\n",
        "- [Pandas Best Practices](https://youtu.be/example3) - Tips avanzados\n",
        "\n",
        "### Lecturas\n",
        "- [Pandas Documentation](https://pandas.pydata.org/docs/) - Documentación oficial\n",
        "- [10 Minutes to Pandas](https://pandas.pydata.org/docs/user_guide/10min.html) - Guía rápida\n",
        "- [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) - Referencia rápida\n"
      ],
      "metadata": {
        "id": "H3jM8qQQI_lR"
      }
    }
  ]
}