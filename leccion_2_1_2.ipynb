{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO01r9UMiYD4v7mh0+F0PZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/financieras/big_data/blob/main/leccion_2_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecci√≥n 2.1.2: Manipulaci√≥n de datos con Pandas (lectura, limpieza, profiling)\n",
        "\n",
        "## 1. Pandas: El bistur√≠ del data scientist\n",
        "\n",
        "Pandas es para los datos lo que un bistur√≠ para un cirujano: **la herramienta precisa que transforma lo complejo en manejable**. No es exageraci√≥n‚Äîel 80% del trabajo en datos pasa por manipular tablas, y Pandas domina este arte.\n",
        "\n",
        "> **Idea clave:** Pandas convierte datos ca√≥ticos en informaci√≥n estructurada lista para an√°lisis.\n",
        "\n",
        "**¬øPor qu√© Pandas es indispensable?**\n",
        "- üìä Maneja datos tabulares de manera intuitiva (DataFrames)\n",
        "- ‚ö° Operaciones vectorizadas: r√°pidas como C, simples como Python\n",
        "- üîß Integraci√≥n perfecta con el ecosistema de datos\n",
        "- ü§ù Comunidad masiva y documentaci√≥n excelente\n",
        "\n",
        "**Ejemplo visual de transformaci√≥n:**\n",
        "\n",
        "**Antes: Datos desordenados en CSV**\n",
        "```\n",
        "nombre,edad,ciudad,salario\n",
        "Ana,28,Madrid,45000\n",
        "Juan,32,Barcelona,52000\n",
        "Mar√≠a,,Valencia,38000\n",
        "Luis,29,Madrid,41000\n",
        "```\n",
        "\n",
        "**Despu√©s: DataFrame limpio y estructurado**\n",
        "| nombre | edad | ciudad    | salario |\n",
        "|--------|------|-----------|---------|\n",
        "| Ana    | 28   | Madrid    | 45000   |\n",
        "| Juan   | 32   | Barcelona | 52000   |\n",
        "| Mar√≠a  | 29*  | Valencia  | 38000   |\n",
        "| Luis   | 29   | Madrid    | 41000   |\n",
        "\n",
        "*Nulo imputado con mediana\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Lectura de datos: Tu punto de partida\n",
        "\n",
        "### **Formatos esenciales y c√≥mo leerlos**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# CSV (el caballo de batalla)\n",
        "df = pd.read_csv('datos.csv', encoding='utf-8')\n",
        "\n",
        "# Excel (el favorito del negocio)\n",
        "df = pd.read_excel('reporte.xlsx', sheet_name='Ventas')\n",
        "\n",
        "# JSON (APIs y web)\n",
        "df = pd.read_json('datos_api.json')\n",
        "\n",
        "# Desde SQL\n",
        "import sqlite3\n",
        "conn = sqlite3.connect('database.db')\n",
        "df = pd.read_sql('SELECT * FROM ventas', conn)\n",
        "\n",
        "# Desde URL directa\n",
        "df = pd.read_csv('https://ejemplo.com/datos.csv')\n",
        "\n",
        "# Desde el portapapeles (¬°s√∫per √∫til!)\n",
        "df = pd.read_clipboard()\n",
        "```\n",
        "\n",
        "### **Par√°metros que salvan vidas**\n",
        "\n",
        "```python\n",
        "df = pd.read_csv('datos_sucios.csv',\n",
        "                 encoding='latin-1',       # Para caracteres especiales (√±, √°)\n",
        "                 sep=';',                  # Separador diferente a coma\n",
        "                 decimal=',',              # Decimales europeos\n",
        "                 na_values=['NULL', 'N/A', ''],  # Valores faltantes personalizados\n",
        "                 dtype={'telefono': str},  # Forzar tipo de datos\n",
        "                 parse_dates=['fecha'],    # Convertir a fecha autom√°ticamente\n",
        "                 nrows=1000)              # Solo primeras 1000 filas (testing)\n",
        "```\n",
        "\n",
        "**Caso real cr√≠tico:** Un analista recibe datos de ventas europeos con decimales con coma. Sin `decimal=','`, el precio **1.200,50‚Ç¨** se interpreta como **1.20050‚Ç¨**. ¬°Error catastr√≥fico que puede costar millones!\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Exploraci√≥n inicial: Los primeros 5 minutos\n",
        "\n",
        "**Estos son los comandos que debes ejecutar SIEMPRE:**\n",
        "\n",
        "```python\n",
        "# Los 5 comandos esenciales\n",
        "print(df.shape)          # Dimensiones (filas, columnas)\n",
        "print(df.info())         # Tipos de datos, memoria, nulos\n",
        "print(df.head())         # Primeras 5 filas\n",
        "print(df.describe())     # Estad√≠sticas de columnas num√©ricas\n",
        "print(df.columns.tolist())  # Lista de nombres de columnas\n",
        "```\n",
        "\n",
        "### **Inspecci√≥n avanzada**\n",
        "\n",
        "```python\n",
        "# Muestra aleatoria (mejor que head() para datasets grandes)\n",
        "df.sample(10)\n",
        "\n",
        "# Valores √∫nicos por columna\n",
        "df.nunique()\n",
        "\n",
        "# Verificar duplicados\n",
        "print(f\"Duplicados: {df.duplicated().sum()}\")\n",
        "\n",
        "# Memoria utilizada\n",
        "print(f\"Memoria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Limpieza de datos: De sucio a brillante\n",
        "\n",
        "### **Detecci√≥n y manejo de valores faltantes**\n",
        "\n",
        "```python\n",
        "# Diagn√≥stico completo\n",
        "print(\"Valores nulos por columna:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\nPorcentaje de nulos:\")\n",
        "print((df.isnull().mean() * 100).round(2))\n",
        "\n",
        "# Estrategias seg√∫n el contexto\n",
        "# 1. Eliminar (cuando son pocos y aleatorios)\n",
        "df_clean = df.dropna(subset=['columna_critica'])\n",
        "\n",
        "# 2. Imputar con media/mediana (variables num√©ricas)\n",
        "df['edad'] = df['edad'].fillna(df['edad'].median())\n",
        "\n",
        "# 3. Imputar con moda (variables categ√≥ricas)\n",
        "df['ciudad'] = df['ciudad'].fillna(df['ciudad'].mode()[0])\n",
        "\n",
        "# 4. Imputar por grupos (m√°s inteligente)\n",
        "df['precio'] = df.groupby('categoria')['precio'].transform(\n",
        "    lambda x: x.fillna(x.median())\n",
        ")\n",
        "\n",
        "# 5. Forward fill (datos temporales)\n",
        "df['ventas'] = df['ventas'].fillna(method='ffill')\n",
        "```\n",
        "\n",
        "### **Correcci√≥n de tipos de datos**\n",
        "\n",
        "```python\n",
        "# Los tipos incorrectos son bombas de tiempo\n",
        "# Conversiones esenciales\n",
        "df['fecha'] = pd.to_datetime(df['fecha'], errors='coerce')\n",
        "df['precio'] = pd.to_numeric(df['precio'], errors='coerce')\n",
        "df['categoria'] = df['categoria'].astype('category')  # Ahorra memoria\n",
        "\n",
        "# Verificaci√≥n\n",
        "print(\"Tipos despu√©s de la correcci√≥n:\")\n",
        "print(df.dtypes)\n",
        "```\n",
        "\n",
        "### **Limpieza de texto y estandarizaci√≥n**\n",
        "\n",
        "```python\n",
        "# Texto inconsistente es un dolor de cabeza com√∫n\n",
        "df['nombre'] = df['nombre'].str.strip().str.title()\n",
        "df['email'] = df['email'].str.lower()\n",
        "df['telefono'] = df['telefono'].str.replace(r'[\\s-]', '', regex=True)\n",
        "\n",
        "# Estandarizar categor√≠as\n",
        "mapeo_ciudades = {'MAD': 'Madrid', 'mad': 'Madrid', 'MADRID': 'Madrid'}\n",
        "df['ciudad'] = df['ciudad'].map(mapeo_ciudades).fillna(df['ciudad'])\n",
        "```\n",
        "\n",
        "### **Eliminaci√≥n de duplicados**\n",
        "\n",
        "```python\n",
        "# Duplicados exactos\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Duplicados en columnas clave\n",
        "df = df.drop_duplicates(subset=['id_usuario', 'fecha'])\n",
        "\n",
        "# Mantener el √∫ltimo registro\n",
        "df = df.drop_duplicates(subset=['id'], keep='last')\n",
        "\n",
        "# Ver duplicados antes de eliminar\n",
        "duplicados = df[df.duplicated(keep=False)]\n",
        "print(f\"Encontrados {len(duplicados)} registros duplicados\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Transformaciones esenciales\n",
        "\n",
        "### **Selecci√≥n y filtrado**\n",
        "\n",
        "```python\n",
        "# Seleccionar columnas\n",
        "df[['producto', 'precio']]\n",
        "\n",
        "# Filtrar filas\n",
        "df[df['precio'] > 100]\n",
        "df[(df['precio'] > 50) & (df['rating'] >= 4.0)]  # M√∫ltiples condiciones\n",
        "\n",
        "# Query SQL-like (m√°s legible)\n",
        "df.query('precio > 100 and categoria == \"Ropa\"')\n",
        "```\n",
        "\n",
        "### **Creaci√≥n de nuevas columnas**\n",
        "\n",
        "```python\n",
        "# Columna simple\n",
        "df['precio_total'] = df['precio'] * df['cantidad']\n",
        "\n",
        "# Columna condicional con np.where\n",
        "import numpy as np\n",
        "df['descuento'] = np.where(df['precio'] > 100, df['precio'] * 0.10, 0)\n",
        "\n",
        "# M√∫ltiples condiciones con np.select\n",
        "condiciones = [\n",
        "    (df['precio'] < 50),\n",
        "    (df['precio'] >= 50) & (df['precio'] < 100),\n",
        "    (df['precio'] >= 100)\n",
        "]\n",
        "categorias = ['B√°sico', 'Medio', 'Premium']\n",
        "df['segmento'] = np.select(condiciones, categorias)\n",
        "```\n",
        "\n",
        "### **Agregaciones y agrupaciones**\n",
        "\n",
        "```python\n",
        "# Agrupar y agregar\n",
        "df.groupby('categoria')['precio'].mean()\n",
        "\n",
        "# M√∫ltiples agregaciones\n",
        "df.groupby('categoria').agg({\n",
        "    'precio': ['mean', 'min', 'max'],\n",
        "    'cantidad': 'sum'\n",
        "})\n",
        "\n",
        "# Pivot tables\n",
        "df.pivot_table(values='precio',\n",
        "               index='categoria',\n",
        "               columns='ciudad',\n",
        "               aggfunc='mean')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Profiling: Conoce a tus datos en profundidad\n",
        "\n",
        "### **Funci√≥n de an√°lisis de distribuci√≥n**\n",
        "\n",
        "```python\n",
        "def analizar_distribucion(df, columna):\n",
        "    \"\"\"An√°lisis estad√≠stico completo de una variable\"\"\"\n",
        "    stats = {\n",
        "        'media': df[columna].mean(),\n",
        "        'mediana': df[columna].median(),\n",
        "        'moda': df[columna].mode()[0] if not df[columna].mode().empty else None,\n",
        "        'std': df[columna].std(),\n",
        "        'q1': df[columna].quantile(0.25),\n",
        "        'q3': df[columna].quantile(0.75),\n",
        "        'iqr': df[columna].quantile(0.75) - df[columna].quantile(0.25),\n",
        "        'skewness': df[columna].skew(),\n",
        "        'kurtosis': df[columna].kurtosis()\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "# Uso\n",
        "stats_ventas = analizar_distribucion(df, 'ventas')\n",
        "print(stats_ventas)\n",
        "```\n",
        "\n",
        "### **Funci√≥n de reporte de calidad**\n",
        "\n",
        "```python\n",
        "def reporte_calidad(df):\n",
        "    \"\"\"Reporte completo de calidad de datos\"\"\"\n",
        "    calidad = pd.DataFrame({\n",
        "        'tipo_dato': df.dtypes,\n",
        "        'valores_no_nulos': df.count(),\n",
        "        'valores_nulos': df.isnull().sum(),\n",
        "        'porcentaje_nulos': (df.isnull().mean() * 100).round(2),\n",
        "        'valores_unicos': df.nunique(),\n",
        "        'memoria_mb': (df.memory_usage(deep=True) / 1024**2).round(2)\n",
        "    })\n",
        "    return calidad\n",
        "\n",
        "# Reporte ejecutivo\n",
        "print(reporte_calidad(df))\n",
        "```\n",
        "\n",
        "### **Profiling automatizado (opcional)**\n",
        "\n",
        "```python\n",
        "# Pandas Profiling - Reporte HTML completo\n",
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "profile = ProfileReport(df, title=\"An√°lisis de Ventas\")\n",
        "profile.to_file(\"reporte_eda.html\")\n",
        "\n",
        "# Incluye: estad√≠sticas, distribuciones, correlaciones,\n",
        "# valores faltantes, duplicados, alertas de calidad\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Caso pr√°ctico completo: E-commerce\n",
        "\n",
        "**Problema:** Dataset de 10,000 pedidos con m√∫ltiples problemas de calidad.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. LECTURA con par√°metros espec√≠ficos\n",
        "pedidos = pd.read_csv('pedidos_ecommerce.csv',\n",
        "                     sep=';',\n",
        "                     decimal=',',\n",
        "                     parse_dates=['fecha_pedido', 'fecha_envio'],\n",
        "                     encoding='latin-1')\n",
        "\n",
        "# 2. DIAGN√ìSTICO INICIAL\n",
        "print(\"=== DIAGN√ìSTICO INICIAL ===\")\n",
        "print(f\"Shape: {pedidos.shape}\")\n",
        "print(f\"Memoria: {pedidos.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "print(\"\\nValores nulos:\")\n",
        "print(pedidos.isnull().sum())\n",
        "print(f\"\\nDuplicados: {pedidos.duplicated().sum()}\")\n",
        "\n",
        "# 3. LIMPIEZA SISTEM√ÅTICA\n",
        "\n",
        "# Correcci√≥n de precios negativos o cero\n",
        "pedidos = pedidos[pedidos['precio'] > 0]\n",
        "\n",
        "# Imputaci√≥n de categor√≠as faltantes\n",
        "pedidos['categoria'] = pedidos['categoria'].fillna('OTROS')\n",
        "\n",
        "# Estandarizaci√≥n de ciudades\n",
        "pedidos['ciudad'] = pedidos['ciudad'].str.upper().str.strip()\n",
        "\n",
        "# Eliminaci√≥n de duplicados\n",
        "pedidos = pedidos.drop_duplicates(subset=['id_pedido'])\n",
        "\n",
        "# Correcci√≥n de tipos\n",
        "pedidos['telefono'] = pedidos['telefono'].astype(str)\n",
        "\n",
        "# 4. FEATURE ENGINEERING B√ÅSICO\n",
        "pedidos['dias_entrega'] = (\n",
        "    pedidos['fecha_envio'] - pedidos['fecha_pedido']\n",
        ").dt.days\n",
        "pedidos['valor_total'] = pedidos['precio'] * pedidos['cantidad']\n",
        "pedidos['mes'] = pedidos['fecha_pedido'].dt.month\n",
        "pedidos['trimestre'] = pedidos['fecha_pedido'].dt.quarter\n",
        "\n",
        "# 5. VALIDACI√ìN\n",
        "assert pedidos['precio'].min() > 0, \"Hay precios negativos\"\n",
        "assert pedidos['dias_entrega'].min() >= 0, \"Fechas inconsistentes\"\n",
        "\n",
        "# 6. PROFILING FINAL\n",
        "print(\"\\n=== RESULTADO FINAL ===\")\n",
        "print(f\"Shape final: {pedidos.shape}\")\n",
        "print(reporte_calidad(pedidos))\n",
        "\n",
        "# 7. GUARDAR\n",
        "pedidos.to_csv('pedidos_limpio.csv', index=False)\n",
        "pedidos.to_parquet('pedidos_limpio.parquet')  # M√°s eficiente\n",
        "```\n",
        "\n",
        "**Resultado cuantificado:**\n",
        "- **Antes:** 10,000 filas, 15% de nulos, tipos incorrectos, 150 duplicados\n",
        "- **Despu√©s:** 9,850 filas limpias, 0% nulos en campos cr√≠ticos, 0 duplicados\n",
        "- **Tiempo de limpieza:** 10 minutos vs 2+ horas manual\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Funciones m√°gicas para el d√≠a a d√≠a\n",
        "\n",
        "### **Funci√≥n de limpieza express**\n",
        "\n",
        "```python\n",
        "def limpieza_express(df):\n",
        "    \"\"\"Limpieza r√°pida para datasets comunes\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # Eliminar duplicados\n",
        "    df_clean = df_clean.drop_duplicates()\n",
        "    \n",
        "    # Limpiar columnas de texto\n",
        "    text_cols = df_clean.select_dtypes(include=['object']).columns\n",
        "    for col in text_cols:\n",
        "        df_clean[col] = df_clean[col].str.strip().str.lower()\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# Uso r√°pido\n",
        "df_limpio = limpieza_express(df_original)\n",
        "```\n",
        "\n",
        "### **Pipeline completo de preparaci√≥n**\n",
        "\n",
        "```python\n",
        "def preparar_para_eda(archivo):\n",
        "    \"\"\"Pipeline completo: datos listos para EDA\"\"\"\n",
        "    # 1. Lectura\n",
        "    df = pd.read_csv(archivo)\n",
        "    \n",
        "    # 2. Limpieza b√°sica\n",
        "    df = limpieza_express(df)\n",
        "    \n",
        "    # 3. Profiling inicial\n",
        "    reporte = reporte_calidad(df)\n",
        "    \n",
        "    return df, reporte\n",
        "\n",
        "# Uso en proyecto real\n",
        "datos_limpios, diagnostico = preparar_para_eda('datos_brutos.csv')\n",
        "print(diagnostico)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Buenas pr√°cticas y errores comunes\n",
        "\n",
        "### ‚úÖ **Haz esto:**\n",
        "\n",
        "```python\n",
        "# Trabajar con copias para no modificar datos originales\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Documentar cada transformaci√≥n\n",
        "# NOTA: Imputamos nulos en edad con mediana (distribuci√≥n sesgada)\n",
        "df['edad'] = df['edad'].fillna(df['edad'].median())\n",
        "\n",
        "# Validar resultados con assertions\n",
        "assert df['precio'].min() >= 0, \"ERROR: Hay precios negativos\"\n",
        "assert df.isnull().sum().sum() == 0, \"ERROR: Quedan valores nulos\"\n",
        "\n",
        "# Verificar despu√©s de operaciones cr√≠ticas\n",
        "print(f\"Filas antes: {len(df_original)}\")\n",
        "df_clean = df_original.dropna()\n",
        "print(f\"Filas despu√©s: {len(df_clean)}\")\n",
        "```\n",
        "\n",
        "### ‚ùå **Evita esto:**\n",
        "\n",
        "```python\n",
        "# ‚ùå Modificar el DataFrame original directamente\n",
        "df = df.dropna()  # ¬°Peligro! Pierdes datos originales\n",
        "\n",
        "# ‚ùå Usar bucles cuando hay vectorizaci√≥n\n",
        "for i in range(len(df)):\n",
        "    df.loc[i, 'nueva'] = df.loc[i, 'col1'] * 2  # LENTO\n",
        "\n",
        "# ‚úÖ Usa operaciones vectorizadas\n",
        "df['nueva'] = df['col1'] * 2  # R√ÅPIDO\n",
        "\n",
        "# ‚ùå Ignorar los warnings\n",
        "# Investiga siempre SettingWithCopyWarning y FutureWarning\n",
        "```\n",
        "\n",
        "### **Checklist de manipulaci√≥n**\n",
        "\n",
        "- [ ] Datos le√≠dos correctamente (encoding, separadores, tipos)\n",
        "- [ ] Exploraci√≥n inicial completada (`info()`, `describe()`)\n",
        "- [ ] Valores nulos identificados y tratados\n",
        "- [ ] Duplicados eliminados o justificados\n",
        "- [ ] Texto estandarizado y limpio\n",
        "- [ ] Tipos de datos correctos por columna\n",
        "- [ ] Outliers identificados y validados\n",
        "- [ ] Transformaciones documentadas con comentarios\n",
        "- [ ] Validaciones con assertions ejecutadas\n",
        "- [ ] Dataset guardado en formato eficiente\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Resumen\n",
        "\n",
        "**Pandas es:**\n",
        "- ‚úÖ El **est√°ndar** para manipulaci√≥n de datos en Python\n",
        "- ‚úÖ **Vers√°til:** Lee cualquier formato (CSV, Excel, JSON, SQL)\n",
        "- ‚úÖ **Potente:** Limpieza, transformaci√≥n, agregaci√≥n\n",
        "- ‚úÖ **R√°pido:** Operaciones vectorizadas\n",
        "\n",
        "**Flujo t√≠pico de trabajo:**\n",
        "```\n",
        "1. Leer ‚Üí read_csv(), read_excel()\n",
        "2. Explorar ‚Üí info(), describe(), head()\n",
        "3. Limpiar ‚Üí Nulos, duplicados, tipos\n",
        "4. Transformar ‚Üí Filtrar, agrupar, nuevas columnas\n",
        "5. Validar ‚Üí Assertions, profiling\n",
        "6. Guardar ‚Üí to_csv(), to_parquet()\n",
        "```\n",
        "\n",
        "**Las 3 operaciones m√°s usadas:**\n",
        "1. `df[df['columna'] > valor]` ‚Üí Filtrar\n",
        "2. `df.groupby('categoria')['valor'].mean()` ‚Üí Agrupar\n",
        "3. `df['nueva'] = df['col1'] * df['col2']` ‚Üí Crear columnas\n",
        "\n",
        "> **Conclusi√≥n:** Dominar Pandas es dominar el 80% del trabajo diario de un Data Analyst. Invierte tiempo en aprenderlo bien‚Äîcada minuto pagar√° dividendos exponenciales.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Referencias\n",
        "\n",
        "### V√≠deos\n",
        "- [Complete Pandas Tutorial](https://youtu.be/vmEHCJofslg) - Tutorial completo\n",
        "- [Data Cleaning with Pandas](https://youtu.be/example2) - Limpieza pr√°ctica\n",
        "- [Pandas Best Practices](https://youtu.be/example3) - Tips avanzados\n",
        "\n",
        "### Lecturas\n",
        "- [Pandas Documentation](https://pandas.pydata.org/docs/) - Documentaci√≥n oficial\n",
        "- [10 Minutes to Pandas](https://pandas.pydata.org/docs/user_guide/10min.html) - Gu√≠a r√°pida\n",
        "- [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) - Referencia r√°pida\n"
      ],
      "metadata": {
        "id": "H3jM8qQQI_lR"
      }
    }
  ]
}