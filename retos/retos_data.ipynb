{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/financieras/big_data/blob/main/retos/retos_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 120 RETOS DE DATA\n",
        "\n",
        "---\n",
        "\n",
        "## **BLOQUE 1: FUNDAMENTOS Y CARGA DE DATOS (Retos 1-10)**\n",
        "\n",
        "### Reto 1. Gestión de archivos en Google Colab\n",
        "- Objetivo: Familiarizarse con la interfaz de Colab y la gestión básica de archivos\n",
        "- Descarga manualmente el dataset `Titanic` desde Kaggle\n",
        "- Sube el archivo manualmente a Google Colab\n",
        "- Carga el dataset usando Pandas\n",
        "- Visualiza las primeras 5 filas con `.head()`\n",
        "- Monta Google Drive y verifica persistencia\n",
        "\n",
        "### Reto 2. Carga directa desde URL y análisis exploratorio\n",
        "- Carga Titanic desde URL con Pandas\n",
        "- Verifica con `.shape` y `.head()`\n",
        "- Explora tipos de datos con `.dtypes`, valores nulos con `.isnull().sum()` y estadísticas descriptivas con `.describe()`\n",
        "\n",
        "### Reto 3. Datasets embebidos y análisis de supervivencia\n",
        "- Carga Titanic desde Seaborn: `sns.load_dataset('titanic')`\n",
        "- Calcula proporción de supervivientes por sexo usando `.groupby(['sex'])['survived'].mean()`\n",
        "- Calcula proporción de supervivientes por clase\n",
        "- Crea gráfico de barras con estas proporciones\n",
        "\n",
        "### Reto 4. Limpieza básica de datos\n",
        "- Carga Titanic desde Seaborn: `sns.load_dataset('titanic')`\n",
        "- Identifica valores nulos en Titanic con `.isnull().sum()`\n",
        "- Rellena \"age\" con mediana: `df['age'].fillna(df['age'].median(), inplace=True)`\n",
        "- Elimina columna \"deck\": `df.drop('deck', axis=1, inplace=True)`\n",
        "- Crea columna \"family_size\" = sibsp + parch + 1\n",
        "\n",
        "### Reto 5. Filtros y consultas básicas\n",
        "- Carga Titanic desde Seaborn: `sns.load_dataset('titanic')`\n",
        "- Filtra pasajeros de primera clase que sobrevivieron: `df[(df['class']=='First') & (df['survived']==1)]`\n",
        "- Encuentra pasajero más joven con `df.loc[df['age'].idxmin()]`\n",
        "- Encuentra pasajero más viejo\n",
        "- Calcula tarifa promedio por clase con `.groupby('class')['fare'].mean()`\n",
        "\n",
        "### Reto 6. Dataset \"Iris\" desde Seaborn\n",
        "- Carga Iris desde Seaborn: `sns.load_dataset('iris')`\n",
        "- Explora estructura con `.shape`, `.info()`, `.columns`\n",
        "- Verifica valores nulos\n",
        "- Calcula número de observaciones por especie con `.value_counts()`\n",
        "\n",
        "### Reto 7. Estadísticas descriptivas y pairplot\n",
        "- Carga Iris desde Seaborn: `sns.load_dataset('iris')`\n",
        "- Calcula media de todas las columnas numéricas con `.mean()`\n",
        "- Calcula desviación estándar con `.std()`\n",
        "- Calcula estadísticas por especie: `df.groupby('species').mean()`\n",
        "- Crea pairplot coloreado por especie: `sns.pairplot(df, hue='species')`\n",
        "\n",
        "### Reto 8. Correlaciones y mapa de calor\n",
        "- Carga Iris desde Seaborn: `sns.load_dataset('iris')`\n",
        "- Selecciona solo columnas numéricas de Iris\n",
        "- Calcula matriz de correlación con `.corr()`\n",
        "- Visualiza con heatmap: `sns.heatmap(corr, annot=True, cmap='coolwarm')`\n",
        "- Identifica las dos variables más correlacionadas\n",
        "\n",
        "### Reto 9. Clasificación con regla simple\n",
        "- Carga Iris desde Seaborn: `sns.load_dataset('iris')`\n",
        "- Crea regla: si petal_length < 2.5 → setosa, si < 5.0 → versicolor, sino → virginica\n",
        "- Crea nueva columna 'prediccion' con esta regla\n",
        "- Compara con columna 'species' real\n",
        "- Calcula precisión: `(df['prediccion'] == df['species']).mean()`\n",
        "\n",
        "### Reto 10. Visualización avanzada con boxplot y violinplot\n",
        "- Carga Iris desde Seaborn: `sns.load_dataset('iris')`\n",
        "- Crea boxplot de sepal_width por especie\n",
        "- Crea violinplot de la misma variable\n",
        "- Identifica especie con mayor mediana de sepal_width\n",
        "- Identifica especie con mayor variabilidad\n",
        "\n",
        "---\n",
        "\n",
        "## **BLOQUE 2: ANÁLISIS EXPLORATORIO Y VISUALIZACIÓN (Retos 11-25)**\n",
        "\n",
        "### Reto 11. Dataset \"Breast Cancer\" desde scikit-learn\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "df['diagnosis'] = df['target'].map({0: 'malignant', 1: 'benign'})\n",
        "```\n",
        "- Explora dimensiones con `.shape`\n",
        "- Verifica balance entre clases con `.value_counts()`\n",
        "- Calcula porcentaje de cada diagnóstico\n",
        "\n",
        "### Reto 12. Análisis de características médicas\n",
        "- Carga el dataset \"Breast Cancer\" desde scikit-learn\n",
        "- Calcula estadísticas descriptivas por diagnóstico: `df.groupby('diagnosis').mean()`\n",
        "- Identifica las 5 características con mayor diferencia entre malignos y benignos\n",
        "- Crea histograma de 'mean radius' separado por diagnóstico usando `hue='diagnosis'`\n",
        "\n",
        "### Reto 13. Correlaciones en datos médicos\n",
        "- Carga el dataset \"Breast Cancer\" desde scikit-learn\n",
        "- Calcula matriz de correlación de las primeras 10 características\n",
        "- Encuentra qué características están más correlacionadas con 'worst radius'\n",
        "- Visualiza con heatmap las primeras 10 características\n",
        "\n",
        "### Reto 14. Análisis de características musicales\n",
        "- Carga el dataset: `df = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')`\n",
        "- Explora variables (danceability, energy, tempo, popularity, genre)\n",
        "- Calcula popularidad promedio por género: `df.groupby('playlist_genre')['track_popularity'].mean()`\n",
        "- Identifica correlación entre danceability y energy con `.corr()`\n",
        "- Crea boxplot de tempo por género\n",
        "\n",
        "### Reto 15. Análisis de patrones de tarifas de taxi\n",
        "- Carga el dataset \"NYC Yellow Taxi Trip Data\": `df = pd.read_csv('yellow_tripdata_2016-01.csv', nrows=100000)` (usa subconjunto para eficiencia)\n",
        "- Extrae hora de recogida: `df['pickup_hour'] = pd.to_datetime(df['tpep_pickup_datetime']).dt.hour`\n",
        "- Calcula tarifa promedio por hora del día: `df.groupby('pickup_hour')['fare_amount'].mean()`\n",
        "- Calcula tarifa promedio por hora y número de pasajeros: `df.groupby(['pickup_hour', 'passenger_count'])['fare_amount'].mean()`\n",
        "- Identifica qué combinación hora-pasajeros tiene tarifa más alta\n",
        "- Crea boxplot de fare_amount por hora del día\n",
        "\n",
        "### Reto 16. Relación cuenta-propina\n",
        "- Carga el dataset: `sns.load_dataset('tips')`\n",
        "- Crea scatterplot de total_bill vs tip\n",
        "- Calcula correlación entre ambas variables con `.corr()`\n",
        "- Crea scatterplot separado por sexo usando `hue='sex'`\n",
        "- ¿Qué sexo muestra mayor correlación?\n",
        "\n",
        "### Reto 17. Análisis de libros desde Open Library API\n",
        "- Carga datos de libros desde API: `url = \"https://openlibrary.org/subjects/fantasy.json\"`\n",
        "- Convierte a DataFrame: `df = pd.DataFrame(data['works'])`\n",
        "- Crea 'title_length' = número de caracteres del título: `df['title_length'] = df['title'].str.len()`\n",
        "- Crea 'authors_count' = número de autores: `df['authors_count'] = df['authors'].apply(len)`\n",
        "- Calcula rating promedio por década de publicación\n",
        "- Identifica década con libros mejor calificados\n",
        "- Crea scatterplot de title_length vs rating\n",
        "\n",
        "### Reto 18. Ordenación y selección\n",
        "- Carga Iris desde Seaborn: `sns.load_dataset('iris')`\n",
        "- Ordena Iris por petal_length de menor a mayor con `.sort_values()`\n",
        "- Muestra las primeras 10 filas con `.head(10)`\n",
        "- Muestra las últimas 10 filas con `.tail(10)`\n",
        "- Extrae solo columnas de pétalos: `df[['petal_length', 'petal_width']]`\n",
        "\n",
        "### Reto 19. Agrupaciones y agregaciones con datos de SpaceX\n",
        "- Carga datos de lanzamientos desde API: `url = \"https://api.spacexdata.com/v4/launches\"`\n",
        "- Convierte a DataFrame: `df = pd.json_normalize(data)`\n",
        "- Usa groupby para calcular éxito promedio, máximo y mínimo de `flight_number` por año\n",
        "- Cuenta número de lanzamientos por compañía: `df['rocket.company'].value_counts()`\n",
        "- Crea gráfico de barras con número de lanzamientos por año\n",
        "- Añade título \"Lanzamientos de SpaceX por año\" y etiquetas a los ejes\n",
        "\n",
        "### Reto 20. Filtrado combinado\n",
        "- Carga Iris desde Seaborn: `sns.load_dataset('iris')`\n",
        "- Filtra flores con sepal_length > 6 AND petal_width < 1.5\n",
        "- Cuenta cuántas flores cumplen la condición\n",
        "- Identifica qué especies están en este subconjunto\n",
        "- Crea scatterplot de sepal_length vs petal_width de estas flores\n",
        "\n",
        "### Reto 21. Normalización de datos económicos (World Bank API)\n",
        "- Carga datos de PIB desde API: `url = \"http://api.worldbank.org/v2/country/ES;FR;DE;IT;GB/indicator/NY.GDP.MKTP.CD?format=json&per_page=100\"`\n",
        "- Convierte a DataFrame: `df = pd.json_normalize(data[1])`\n",
        "- Normaliza el valor del PIB (`value`) usando fórmula: (x - min) / (max - min)\n",
        "- Verifica que los valores estén entre 0 y 1 con `.min()` y `.max()`\n",
        "- Normaliza también el año (`date`) como años desde el primero\n",
        "- Crea scatterplot de año normalizado vs PIB normalizado por país\n",
        "\n",
        "### Reto 22. Dataset Penguins desde Seaborn\n",
        "- Carga: `sns.load_dataset('penguins')`\n",
        "- Explora valores nulos por columna\n",
        "- Elimina filas con valores nulos: `df.dropna()`\n",
        "- Identifica las 3 especies diferentes\n",
        "\n",
        "### Reto 23. Análisis morfológico de pingüinos\n",
        "- Carga: `sns.load_dataset('penguins')`\n",
        "- Calcula peso promedio (body_mass_g) por especie\n",
        "- Calcula peso promedio por especie y sexo\n",
        "- Identifica especie con mayor peso promedio\n",
        "- Crea scatterplot de flipper_length_mm vs body_mass_g coloreado por especie\n",
        "\n",
        "### Reto 24. Distribución geográfica de pingüinos\n",
        "- Carga: `sns.load_dataset('penguins')`\n",
        "- Crea boxplot de body_mass_g por island\n",
        "- Identifica isla con pingüinos más pesados en promedio\n",
        "- Crea tabla cruzada de especies por isla: `pd.crosstab(df['species'], df['island'])`\n",
        "- ¿Qué especie está en las tres islas?\n",
        "\n",
        "### Reto 25. Exportación a múltiples formatos\n",
        "- Carga Iris desde Seaborn: `sns.load_dataset('iris')`\n",
        "- Guarda Iris (con columnas calculadas) en CSV: `df.to_csv('iris_modified.csv', index=False)`\n",
        "- Guarda en Excel: `df.to_excel('iris_modified.xlsx', index=False)`\n",
        "- Guarda en JSON: `df.to_json('iris_modified.json', orient='records')`\n",
        "- Verifica carga correcta de cada formato\n",
        "\n",
        "---\n",
        "\n",
        "## **BLOQUE 3: OPERACIONES CON DATOS Y TRANSFORMACIONES (Retos 26-45)**\n",
        "\n",
        "### Reto 26. Tablas cruzadas en datos de streaming\n",
        "- Carga datos desde API: `url = \"http://api.tvmaze.com/schedule\"`\n",
        "- Convierte a DataFrame: `df = pd.json_normalize(data)`\n",
        "- Crea tabla cruzada entre tipo de programa (`show.type`) y canal (`show.network.name`)\n",
        "- Crea tabla cruzada con rating promedio como valores y aggfunc='mean'\n",
        "- Interpreta: ¿qué combinación tipo-canal tiene mayor rating promedio?\n",
        "- Visualiza con heatmap\n",
        "\n",
        "### Reto 27. Dataset Diamonds desde Seaborn\n",
        "- Carga: `sns.load_dataset('diamonds')`\n",
        "- Explora dimensiones y primeras filas\n",
        "- Identifica variables categóricas: cut, color, clarity\n",
        "- Calcula estadísticas descriptivas de price\n",
        "\n",
        "### Reto 28. Análisis de precios por características\n",
        "- Carga: `df = pd.read_csv('http://data.insideairbnb.com/united-states/wa/seattle/2023-09-05/data/listings.csv', nrows=10000)`\n",
        "- Calcula precio promedio por room_type\n",
        "- Calcula precio promedio por neighbourhood\n",
        "- Identifica combinación neighbourhood-room_type más costosa con `.groupby(['neighbourhood','room_type'])['price'].mean()`\n",
        "- Crea scatterplot de minimum_nights vs price coloreado por room_type\n",
        "\n",
        "### Reto 29. Detección y eliminación de duplicados en datos de países\n",
        "- Carga datos desde REST Countries API: `url = \"https://restcountries.com/v3.1/all\"`\n",
        "- Convierte a DataFrame: `df = pd.json_normalize(data)`\n",
        "- Duplica artificialmente primeras 15 filas: `df = pd.concat([df, df.head(15)])`\n",
        "- Cuenta duplicados por\n",
        "\n",
        "### Reto 30. Dataset Flights desde Seaborn\n",
        "- Carga: `sns.load_dataset('flights')`\n",
        "- Explora estructura temporal (year, month, passengers)\n",
        "- Identifica rango de años incluido\n",
        "- Identifica todos los meses únicos\n",
        "\n",
        "### Reto 31. Análisis de tráfico aéreo\n",
        "- Carga: `sns.load_dataset('flights')`\n",
        "- Agrupa por año y calcula total de pasajeros: `df.groupby('year')['passengers'].sum()`\n",
        "- Crea gráfico de líneas de evolución temporal\n",
        "- Calcula tasa de crecimiento entre primer y último año\n",
        "- ¿En qué año se superaron los 400 pasajeros totales?\n",
        "\n",
        "### Reto 32. Mapa de calor mensual\n",
        "- Carga: `df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset/day.csv')`\n",
        "- Crea tabla pivote: `df.pivot(index='mnth', columns='yr', values='cnt')`\n",
        "- Crea heatmap con anotaciones: `sns.heatmap(pivot, annot=True, fmt='d', cmap='YlOrRd')`\n",
        "- Identifica mes con más alquileres consistente a lo largo de los años\n",
        "- ¿Qué mes tiene menos alquileres?\n",
        "\n",
        "### Reto 33. Dataset California Housing desde URL\n",
        "- Carga: `df = pd.read_csv('https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv')`\n",
        "- Explora variables (median_house_value, total_rooms, housing_median_age, population, etc.)\n",
        "- Calcula precio promedio (median_house_value) por antigüedad de vivienda (agrupa housing_median_age en rangos)\n",
        "- Identifica distribución geográfica (latitude, longitude)\n",
        "\n",
        "### Reto 34. Análisis de vehículos eléctricos\n",
        "- Carga el dataset: `df = pd.read_csv('https://raw.githubusercontent.com/datasets/electric-vehicle-population/main/data/electric_vehicle_population.csv')`\n",
        "- Explora variables (electric_range, make, model_year, electric_vehicle_type)\n",
        "- Calcula autonomía promedio (electric_range) por fabricante (make)\n",
        "- Identifica los 5 fabricantes con mayor autonomía promedio\n",
        "- Crea scatterplot de model_year vs electric_range coloreado por electric_vehicle_type\n",
        "\n",
        "## Reto 35. Análisis de Productos Alimenticios\n",
        "```python\n",
        "import requests\n",
        "url = 'https://world.openfoodfacts.org/cgi/search.pl?action=process&json=1&page_size=100'\n",
        "data = requests.get(url).json()\n",
        "df = pd.DataFrame(data['products'])\n",
        "```\n",
        "- Calcula distribución de nutrientes principales (azúcar, grasa, proteína)\n",
        "- Identifica categorías de productos con mayor contenido de azúcar\n",
        "- Agrupa por categoría y calcula promedios nutricionales\n",
        "- Crea scatterplot de azúcar vs grasa por categoría\n",
        "\n",
        "### Reto 36. Tablas pivot avanzadas\n",
        "- Carga: `sns.load_dataset('flights')`\n",
        "- Con flights, crea tabla pivot año-mes como en Reto 32\n",
        "- Calcula pasajeros promedio por mes (promedio entre todos los años)\n",
        "- Identifica julio de 1955: ¿cuántos pasajeros hubo?\n",
        "- Calcula crecimiento de pasajeros entre enero y diciembre de cada año\n",
        "\n",
        "### Reto 37. Operación melt (formato ancho a largo)\n",
        "- Toma la tabla pivot del Reto 36\n",
        "- Aplica melt para volver a formato largo: `pd.melt(pivot.reset_index(), id_vars=['month'])`\n",
        "- Verifica que coincide con estructura original\n",
        "- Cuenta cuántas filas tiene cada formato\n",
        "\n",
        "### Reto 38. Dataset Planets desde Seaborn\n",
        "- Carga: `sns.load_dataset('planets')`\n",
        "- Explora métodos de descubrimiento (method)\n",
        "- Cuenta exoplanetas descubiertos por cada método\n",
        "- Identifica los 5 métodos más comunes\n",
        "\n",
        "### Reto 39. Análisis de exoplanetas\n",
        "- Carga: `sns.load_dataset('planets')`\n",
        "- Calcula masa promedio por método (ignora nulos)\n",
        "- Calcula año promedio de descubrimiento por método\n",
        "- Usa `.agg()` para calcular múltiples funciones: `df.groupby('method').agg({'mass': 'mean', 'year': 'mean', 'number': 'count'})`\n",
        "- ¿Qué método tiene exoplanetas más masivos en promedio?\n",
        "\n",
        "### Reto 40. Combinación con merge\n",
        "- Crea DataFrame auxiliar con información ficticia de islas:\n",
        "```python\n",
        "island_info = pd.DataFrame({\n",
        "    'island': ['Torgersen', 'Biscoe', 'Dream'],\n",
        "    'country': ['Antarctica', 'Antarctica', 'Antarctica'],\n",
        "    'area_km2': [15, 40, 25]\n",
        "})\n",
        "```\n",
        "- Carga: `sns.load_dataset('penguins')`\n",
        "- Combina con merge: `pd.merge(penguins, island_info, on='island')`\n",
        "- Verifica número de filas resultante\n",
        "- Calcula densidad de pingüinos por km2 por isla\n",
        "\n",
        "### Reto 41. Concatenación vertical\n",
        "- Carga el dataset: `sns.load_dataset('tips')`\n",
        "- En tips, filtra solo filas de Saturday: `sat = df[df['day']=='Sat']`\n",
        "- Filtra solo filas de Sunday: `sun = df[df['day']=='Sun']`\n",
        "- Concatena verticalmente: `pd.concat([sat, sun])`\n",
        "- Verifica que total de filas = suma de ambos\n",
        "\n",
        "### Reto 42. Análisis de cuartiles e IQR en criptomonedas\n",
        "- Carga datos desde CoinGecko API: `url = \"https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd&order=market_cap_desc&per_page=100\"`\n",
        "- Convierte a DataFrame: `df = pd.DataFrame(data)`\n",
        "- Calcula Q1, Q2 (mediana), Q3 de `price_change_percentage_24h` con `.quantile([0.25, 0.5, 0.75])`\n",
        "- Calcula IQR = Q3 - Q1\n",
        "- Define límites outliers: inferior = Q1 - 1.5*IQR, superior = Q3 + 1.5*IQR\n",
        "- Cuenta cuántas criptomonedas son outliers en variación de precio\n",
        "- Identifica los nombres de las 5 criptomonedas con mayor variación positiva\n",
        "\n",
        "### Reto 43. Cálculo de sesgo (skewness)\n",
        "- Carga: `sns.load_dataset('diamonds')`\n",
        "- Usa `from scipy.stats import skew`\n",
        "- Calcula sesgo de price en diamonds: `skew(df['price'])`\n",
        "- Interpreta: valor positivo indica asimetría hacia derecha\n",
        "- Crea histograma de price para visualizar el sesgo\n",
        "\n",
        "### Reto 44. Visualización: swarmplot\n",
        "- Carga: `sns.load_dataset('penguins')`\n",
        "- En penguins (sin nulos), crea swarmplot de body_mass_g por species\n",
        "- Compara visualmente con boxplot de la misma variable\n",
        "- Identifica outliers individuales fácilmente\n",
        "- ¿Qué especie tiene mayor dispersión de pesos?\n",
        "\n",
        "### Reto 45. Análisis de Ratings de Películas\n",
        "- Carga el dataset: `df = pd.read_csv('https://raw.githubusercontent.com/alexanderQLE/ML-labs/master/movielens/ratings.csv')`\n",
        "- Explora variables (userId, movieId, rating, timestamp)\n",
        "- Calcula rating promedio por película: `df.groupby('movieId')['rating'].mean()`\n",
        "- Identifica las 10 películas con más ratings\n",
        "- Crea violinplot de ratings por película (top 10 películas)\n",
        "- Compara distribución de ratings entre películas populares\n",
        "\n",
        "---\n",
        "\n",
        "## **BLOQUE 4: SERIES TEMPORALES Y DATASETS COMPLEJOS (Retos 46-60)**\n",
        "\n",
        "### Reto 46. Dataset de Temperaturas Globales (Berkeley Earth)\n",
        "- Carga desde URL: `https://raw.githubusercontent.com/datasets/global-temp/master/data/annual.csv`\n",
        "- Explora columnas: Year, Mean\n",
        "- Filtra datos desde 1900 en adelante\n",
        "- Crea gráfico de líneas de temperatura media por año\n",
        "\n",
        "### Reto 47. Análisis de cambio climático\n",
        "- Carga desde URL: `https://raw.githubusercontent.com/datasets/global-temp/master/data/annual.csv`\n",
        "- Calcula temperatura media del período 1900-1950\n",
        "- Calcula temperatura media del período 1970-2020\n",
        "- Calcula diferencia entre ambos períodos\n",
        "- Identifica el año más cálido registrado desde 1900\n",
        "\n",
        "### Reto 48. Media móvil en series temporales\n",
        "- Carga desde URL: `https://raw.githubusercontent.com/datasets/global-temp/master/data/annual.csv`\n",
        "- Con temperaturas globales, calcula media móvil de 10 años: `df['MA_10'] = df['Mean'].rolling(window=10).mean()`\n",
        "- Crea gráfico con temperatura original y media móvil\n",
        "- ¿La media móvil suaviza las fluctuaciones?\n",
        "- Calcula también media móvil de 30 años\n",
        "\n",
        "### Reto 49. Dataset Global Air Quality\n",
        "- Carga: `df = pd.read_csv('air_quality_data.csv')`\n",
        "- Filtra solo datos de España: `df = df[df['country'] == 'ES']`\n",
        "- Extrae columnas: date, value, parameter\n",
        "- Convierte 'date' a datetime: `df['date'] = pd.to_datetime(df['date'])`\n",
        "\n",
        "### Reto 50. Análisis temporal de viajes en taxi en Nueva York\n",
        "- Carga el dataset \"NYC Yellow Taxi\": `df = pd.read_csv('yellow_tripdata_2016-01.csv', nrows=100000)` (usa subconjunto para eficiencia)\n",
        "- Convierte 'tpep_pickup_datetime' a datetime: `df['pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])`\n",
        "- Extrae fecha: `df['pickup_date'] = df['pickup_datetime'].dt.date`\n",
        "- Calcula viajes diarios: `daily_trips = df.groupby('pickup_date').size()`\n",
        "- Identifica día con más viajes: `daily_trips.idxmax()`\n",
        "- Calcula media móvil de 7 días: `daily_trips.rolling(window=7).mean()`\n",
        "- Crea gráfico de líneas de viajes diarios con media móvil superpuesta\n",
        "\n",
        "### Reto 51. Análisis de sentimiento en tweets\n",
        "- Carga el dataset: `df = pd.read_csv('https://raw.githubusercontent.com/huseinzol05/NLP-Dataset/master/Twitter%20Sentiment%20Data/train.csv')`\n",
        "- Explora variables (text, label) - label: 0=negativo, 1=positivo\n",
        "- Extrae mes y año de timestamps si están disponibles, sino crea fecha artificial\n",
        "- Agrupa por mes y calcula proporción de tweets positivos\n",
        "- Identifica mes con mayor porcentaje de tweets positivos\n",
        "- Crea gráfico de líneas de evolución de sentimiento por mes\n",
        "\n",
        "### Reto 52. Dataset Anscombe desde Seaborn\n",
        "- Carga: `sns.load_dataset('anscombe')`\n",
        "- Explora los 4 datasets (I, II, III, IV)\n",
        "- Calcula media y varianza de x e y para cada dataset\n",
        "- Crea 4 scatterplots (uno por dataset) en la misma figura\n",
        "\n",
        "### Reto 53. Lección estadística de Anscombe\n",
        "- Carga: `sns.load_dataset('anscombe')`\n",
        "- Calcula correlación entre x e y para cada dataset\n",
        "- Observa que las 4 tienen estadísticas casi idénticas\n",
        "- Compara con los gráficos del Reto 52\n",
        "- Identifica dataset con outlier claro y dataset con relación no lineal\n",
        "\n",
        "### Reto 54. Normalización Z-score\n",
        "- Carga Iris desde Seaborn: `sns.load_dataset('iris')`\n",
        "- En Iris, calcula Z-score de sepal_length: `(df['sepal_length'] - df['sepal_length'].mean()) / df['sepal_length'].std()`\n",
        "- Verifica que media ≈ 0 con `.mean()`\n",
        "- Verifica que std ≈ 1 con `.std()`\n",
        "- Compara distribución original vs normalizada con histogramas\n",
        "\n",
        "### Reto 55. Análisis de géneros musicales en Spotify\n",
        "- Carga datos desde Spotify API (simulada): `df = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')`\n",
        "- Extrae año del álbum: `df['release_year'] = pd.to_datetime(df['track_album_release_date']).dt.year`\n",
        "- Cuenta canciones por género musical con `.value_counts()`\n",
        "- Identifica géneros minoritarios (< 50 canciones)\n",
        "- Calcula popularidad promedio por género musical\n",
        "- Visualiza con gráfico de barras los 10 géneros más populares\n",
        "\n",
        "### Reto 56. Dataset Car Crashes desde Seaborn\n",
        "- Carga: `sns.load_dataset('car_crashes')`\n",
        "- Explora variables por estado de USA\n",
        "- Identifica estado con más accidentes totales (total)\n",
        "- Crea ranking de top 10 estados por total de accidentes\n",
        "\n",
        "### Reto 57. Análisis de Población Mundial\n",
        "- Carga desde API:\n",
        "```python\n",
        "import requests\n",
        "url = 'http://api.worldbank.org/v2/country/all/indicator/SP.POP.TOTL?format=json'\n",
        "data = requests.get(url).json()\n",
        "df = pd.json_normalize(data[1])\n",
        "```\n",
        "- Explora variables (country.value, value, date)\n",
        "- Filtra datos del año más reciente\n",
        "- Identifica los 10 países más poblados\n",
        "- Calcula correlación entre año y población para un país seleccionado\n",
        "- Crea scatterplot de población por año para países top 5\n",
        "\n",
        "### Reto 58. Dataset de Poblaciones (Gapminder)\n",
        "- Carga: `sns.load_dataset('gapminder')`\n",
        "- Explora variables: country, continent, year, lifeExp, pop, gdpPercap\n",
        "- Identifica países y años incluidos\n",
        "- Calcula población mundial por año\n",
        "\n",
        "### Reto 59. Análisis de felicidad por región\n",
        "- Carga: `df = pd.read_csv('https://raw.githubusercontent.com/Opensourcefordatascience/Data-sets/master/2019.csv')`\n",
        "- Filtra solo datos del año 2019\n",
        "- Agrupa por región y calcula promedios de Happiness Score y GDP per capita\n",
        "- Identifica región con mayor felicidad promedio\n",
        "- Calcula correlación entre GDP per capita y Happiness Score\n",
        "\n",
        "### Reto 60. Evolución temporal en Gapminder\n",
        "- Carga: `sns.load_dataset('gapminder')`\n",
        "- Filtra datos de Spain a lo largo del tiempo\n",
        "- Crea gráfico de líneas de lifeExp por año\n",
        "- Crea gráfico de líneas de gdpPercap por año\n",
        "- Calcula crecimiento porcentual de gdpPercap entre 1952 y 2007\n",
        "\n",
        "---\n",
        "\n",
        "## **BLOQUE 5: DATOS DEL MUNDO REAL Y WEB (Retos 61-80)**\n",
        "\n",
        "### Reto 61. Dataset Wine Quality desde UCI\n",
        "- Carga desde URL: `https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv`\n",
        "- Nota: usar `sep=';'` en read_csv\n",
        "- Explora características físico-químicas\n",
        "- Identifica vinos con quality > 7\n",
        "\n",
        "### Reto 62. Correlaciones en vinos\n",
        "- Carga desde URL: `https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv`\n",
        "- Calcula matriz de correlación de todas las variables\n",
        "- Crea heatmap con anotaciones\n",
        "- Identifica variable más correlacionada con quality\n",
        "- ¿alcohol está correlacionado con quality?\n",
        "\n",
        "### Reto 63. Dataset de Estudiantes (UCI Student Performance)\n",
        "- Carga desde URL: `https://raw.githubusercontent.com/uci-ml-repo/ucimlrepo/main/docs/assets/student_performance_math.csv`\n",
        "- Explora variables demográficas y académicas\n",
        "- Calcula nota promedio (G3) por sexo\n",
        "- Calcula nota promedio por nivel educativo de los padres (Medu, Fedu)\n",
        "\n",
        "### Reto 64. Análisis de factores nutricionales en alimentos\n",
        "- Carga datos desde Open Food Facts API: `url = \"https://world.openfoodfacts.org/api/v2/search?countries=spain&page_size=100\"`\n",
        "- Convierte a DataFrame: `df = pd.json_normalize(data['products'])`\n",
        "- Crea variable 'sugar_level' categorizando `nutriments.sugars_100g` en 'bajo' (<5g), 'medio' (5-15g), 'alto' (>15g)\n",
        "- Compara rating promedio (`product_quality`) por categoría de azúcar\n",
        "- Crea boxplot de nutrición score por sugar_level\n",
        "- ¿Mayor contenido de azúcar correlaciona con mejor rating?\n",
        "\n",
        "### Reto 65. Dataset de Países (REST Countries API simulado)\n",
        "```python\n",
        "# Usar dataset pre-descargado de países\n",
        "url = 'https://raw.githubusercontent.com/samayo/country-json/master/src/country-by-population.json'\n",
        "df = pd.read_json(url)\n",
        "```\n",
        "- Explora estructura: country, population\n",
        "- Identifica los 10 países más poblados\n",
        "- Calcula población mundial total\n",
        "- Calcula población promedio por país\n",
        "\n",
        "### Reto 66. Dataset de Capitales\n",
        "```python\n",
        "url = 'https://raw.githubusercontent.com/samayo/country-json/master/src/country-by-capital-city.json'\n",
        "df_capitals = pd.read_json(url)\n",
        "```\n",
        "- Combina con dataset de población del Reto 65\n",
        "- Usa merge para crear dataset completo país-capital-población\n",
        "- Identifica capitales de países más poblados\n",
        "\n",
        "### Reto 67. Dataset Asteroides Cercanos (NASA API)\n",
        "```python\n",
        "import requests\n",
        "url = 'https://api.nasa.gov/neo/rest/v1/neo/browse?api_key=DEMO_KEY'\n",
        "data = requests.get(url).json()\n",
        "df = pd.DataFrame(data['near_earth_objects'])\n",
        "```\n",
        "- Explora variables (name, estimated_diameter, close_approach_data, is_potentially_hazardous_asteroid)\n",
        "- Filtra asteroides potencialmente peligrosos: `df[df['is_potentially_hazardous_asteroid']==True]`\n",
        "- Identifica asteroide con mayor diámetro: `df.loc[df['estimated_diameter']['kilometers']['estimated_diameter_max'].idxmax()]`\n",
        "- Calcula porcentaje de asteroides peligrosos\n",
        "\n",
        "### Reto 68. Análisis sísmico\n",
        "```python\n",
        "url = 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_month.csv'\n",
        "df = pd.read_csv(url)\n",
        "```\n",
        "- Calcula media y desviación estándar de magnitudes\n",
        "- Crea histograma de distribución de magnitudes\n",
        "- Identifica el país/región con más terremotos (extraer de 'place')\n",
        "- Calcula cuántos terremotos ocurrieron en la última semana\n",
        "\n",
        "### Reto 69. Dataset de Intercambio de Divisas\n",
        "```python\n",
        "# Usar API gratuita de exchangerate-api\n",
        "url = 'https://open.er-api.com/v6/latest/USD'\n",
        "import requests\n",
        "data = requests.get(url).json()\n",
        "rates = pd.DataFrame(list(data['rates'].items()), columns=['currency', 'rate'])\n",
        "```\n",
        "- Explora tasas de cambio desde USD\n",
        "- Identifica las 10 monedas con mayor valor frente al USD\n",
        "- Calcula cuántos EUR equivalen a 100 USD\n",
        "\n",
        "### Reto 70. Conversión de monedas\n",
        "```python\n",
        "# Usar API gratuita de exchangerate-api\n",
        "url = 'https://open.er-api.com/v6/latest/USD'\n",
        "import requests\n",
        "data = requests.get(url).json()\n",
        "rates = pd.DataFrame(list(data['rates'].items()), columns=['currency', 'rate'])\n",
        "```\n",
        "- Crea función que convierta cualquier cantidad de USD a otra moneda\n",
        "- Calcula equivalencias de 1000 USD en: EUR, GBP, JPY, MXN\n",
        "- Crea DataFrame con estas conversiones\n",
        "- Ordena monedas por valor de mayor a menor\n",
        "\n",
        "### Reto 71. Dataset de Lenguajes de Programación\n",
        "```python\n",
        "url = 'https://raw.githubusercontent.com/datasets/awesome-data/master/programming-languages.csv'\n",
        "# Si no está disponible, usar alternativa de GitHub similar\n",
        "```\n",
        "- Explora variables: nombre, año de creación, paradigma\n",
        "- Identifica los 10 lenguajes más antiguos\n",
        "- Cuenta lenguajes por década de creación\n",
        "- Identifica paradigma más común\n",
        "\n",
        "### Reto 72. Web Scraping: Tabla HTML simple\n",
        "```python\n",
        "url = 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'\n",
        "tables = pd.read_html(url)\n",
        "df = tables[1]  # Segunda tabla suele ser la correcta\n",
        "```\n",
        "- Extrae tabla de GDP de países desde Wikipedia\n",
        "- Limpia nombres de columnas\n",
        "- Identifica top 10 países por GDP\n",
        "- Calcula GDP promedio de top 20\n",
        "\n",
        "### Reto 73. Dataset de Feriados (Nager.Date API)\n",
        "```python\n",
        "url = 'https://date.nager.at/api/v3/PublicHolidays/2024/ES'\n",
        "df = pd.read_json(url)\n",
        "```\n",
        "- Explora feriados de España en 2024\n",
        "- Cuenta cuántos feriados hay\n",
        "- Identifica mes con más feriados\n",
        "- Extrae solo nombre y fecha de cada feriado\n",
        "\n",
        "### Reto 74. Dataset de Universidades\n",
        "```python\n",
        "url = 'https://raw.githubusercontent.com/Hipo/university-domains-list/master/world_universities_and_domains.json'\n",
        "df = pd.read_json(url)\n",
        "```\n",
        "- Filtra universidades de Spain\n",
        "- Cuenta cuántas universidades españolas hay en el dataset\n",
        "- Identifica las 10 primeras universidades de la lista\n",
        "- Extrae dominios web de universidades españolas\n",
        "\n",
        "### Reto 75. Análisis de Repositorios GitHub\n",
        "- Carga el dataset: `df = pd.read_csv('https://raw.githubusercontent.com/github/platform-samples/master/data/repositories.csv')`\n",
        "- Explora variables (name, language, stars, forks, watchers)\n",
        "- Calcula repositorios por lenguaje: `df.groupby('language').size()`\n",
        "- Identifica los 5 lenguajes con más repositorios\n",
        "- Limpia nombres de lenguajes: convierte a minúsculas y elimina espacios\n",
        "- Crea gráfico de barras de repositorios por lenguaje (top 10)\n",
        "\n",
        "### Reto 76. Datos Meteorológicos en Tiempo Real\n",
        "```python\n",
        "import requests\n",
        "url = 'https://api.open-meteo.com/v1/forecast?latitude=40.4168&longitude=-3.7038&current=temperature_2m,relative_humidity_2m,wind_speed_10m&timezone=Europe/Madrid'\n",
        "data = requests.get(url).json()\n",
        "df = pd.DataFrame([data['current']])\n",
        "```\n",
        "- Explora variables meteorológicas actuales en Madrid (temperature_2m, relative_humidity_2m, wind_speed_10m)\n",
        "- Calcula sensación térmica usando fórmula simplificada: `df['feels_like'] = df['temperature_2m'] - (df['wind_speed_10m'] * 0.7)`\n",
        "- Compara temperatura actual con humedad relativa\n",
        "- Identifica condiciones climáticas predominantes basado en temperatura y viento\n",
        "\n",
        "### Reto 77. Análisis de calidad del aire\n",
        "```python\n",
        "# Usar dataset de calidad del aire\n",
        "url = 'https://data.cityofnewyork.us/api/views/c3uy-2p5r/rows.csv?accessType=DOWNLOAD'\n",
        "df = pd.read_csv(url)\n",
        "```\n",
        "- Agrupa por borough y calcula promedio de contaminantes\n",
        "- Identifica borough con mejor calidad del aire\n",
        "- Crea gráfico de barras comparativo\n",
        "- Filtra días con niveles peligrosos (si aplica)\n",
        "\n",
        "### Reto 78. Dataset de Bibliotecas JSON anidado\n",
        "```python\n",
        "data = {\n",
        "    'students': [\n",
        "        {'name': 'Ana', 'grades': {'math': 85, 'science': 92}},\n",
        "        {'name': 'Luis', 'grades': {'math': 78, 'science': 88}},\n",
        "        {'name': 'María', 'grades': {'math': 95, 'science': 89}}\n",
        "    ]\n",
        "}\n",
        "df = pd.json_normalize(data['students'])\n",
        "```\n",
        "- Aplana estructura JSON anidada\n",
        "- Calcula promedio de cada estudiante\n",
        "- Identifica estudiante con mejor promedio\n",
        "- Calcula nota promedio por materia\n",
        "\n",
        "### Reto 79. Exportación condicional\n",
        "- Carga: `sns.load_dataset('diamonds')`\n",
        "- En diamonds, filtra diamantes con price > 15000\n",
        "- Filtra diamantes \"Ideal\" cut\n",
        "- Combina ambos filtros con operador &\n",
        "- Guarda resultado en CSV: `filtered.to_csv('expensive_ideal_diamonds.csv', index=False)`\n",
        "\n",
        "### Reto 80. Dataset de Pronóstico del Tiempo (Open-Meteo)\n",
        "```python\n",
        "url = 'https://api.open-meteo.com/v1/forecast?latitude=40.4168&longitude=-3.7038&daily=temperature_2m_max,temperature_2m_min&timezone=Europe/Madrid'\n",
        "import requests\n",
        "data = requests.get(url).json()\n",
        "df = pd.DataFrame(data['daily'])\n",
        "```\n",
        "- Extrae pronóstico de 7 días para Madrid\n",
        "- Crea DataFrame con fechas y temperaturas\n",
        "- Calcula temperatura promedio de la semana\n",
        "- Identifica día más cálido y más frío\n",
        "\n",
        "---\n",
        "\n",
        "## **BLOQUE 6: INGENIERÍA DE DATOS BÁSICA (Retos 81-95)**\n",
        "\n",
        "### Reto 81. Simulación de operaciones SQL con Pandas\n",
        "- Carga el dataset: `sns.load_dataset('tips')`\n",
        "- SELECT: selecciona columnas total_bill, tip, day\n",
        "- WHERE: filtra dinner meals: `df[df['time']=='Dinner']`\n",
        "- ORDER BY: ordena por total_bill descendente\n",
        "- Muestra primeras 10 filas del resultado\n",
        "\n",
        "### Reto 82. GROUP BY en Pandas\n",
        "- Carga el dataset: `sns.load_dataset('tips')`\n",
        "- En tips, agrupa por day y time\n",
        "- Calcula: COUNT, AVG(total_bill), SUM(tip) usando `.agg()`\n",
        "- Recrea query SQL: `SELECT day, time, COUNT(*), AVG(total_bill), SUM(tip) FROM tips GROUP BY day, time`\n",
        "- Identifica combinación día-hora con más ventas\n",
        "\n",
        "### Reto 83. JOIN simulado con Pandas\n",
        "- Crea dos DataFrames:\n",
        "```python\n",
        "customers = pd.DataFrame({\n",
        "    'customer_id': [1, 2, 3, 4],\n",
        "    'name': ['Ana', 'Luis', 'María', 'Carlos']\n",
        "})\n",
        "orders = pd.DataFrame({\n",
        "    'order_id': [101, 102, 103],\n",
        "    'customer_id': [1, 2, 1],\n",
        "    'amount': [50, 75, 30]\n",
        "})\n",
        "```\n",
        "- Realiza INNER JOIN: `pd.merge(customers, orders, on='customer_id')`\n",
        "- Realiza LEFT JOIN con `how='left'`\n",
        "- Identifica cliente sin órdenes\n",
        "\n",
        "### Reto 84. Subconsultas con indicadores mundiales\n",
        "- Carga datos desde UN Data API: `url = \"https://unstats.un.org/SDGAPI/v1/sdg/Goal/8/List?includechildren=false\"`\n",
        "- Convierte a DataFrame: `df = pd.json_normalize(data)`\n",
        "- Calcula el PIB per cápita promedio mundial de todos los países\n",
        "- Filtra países con PIB per cápita mayor al promedio mundial\n",
        "- Equivale a SQL: `SELECT * FROM countries WHERE gdp_per_capita > (SELECT AVG(gdp_per_capita) FROM countries)`\n",
        "- Cuenta cuántos países están sobre el promedio y lista los 10 con mayor PIB\n",
        "\n",
        "### Reto 85. HAVING simulado\n",
        "- Carga el dataset: `sns.load_dataset('tips')`\n",
        "- En tips, agrupa por smoker y day\n",
        "- Calcula total_bill promedio por grupo\n",
        "- Filtra solo grupos con promedio > 20\n",
        "- Equivale a: `SELECT smoker, day, AVG(total_bill) FROM tips GROUP BY smoker, day HAVING AVG(total_bill) > 20`\n",
        "\n",
        "### Reto 86. DISTINCT y COUNT DISTINCT\n",
        "- Carga Titanic desde Seaborn: `sns.load_dataset('titanic')`\n",
        "- En titanic, cuenta valores únicos de embark_town: `df['embark_town'].nunique()`\n",
        "- Cuenta combinaciones únicas de class y sex\n",
        "- Usa `.drop_duplicates()` para obtener filas únicas\n",
        "- Compara tamaño original vs sin duplicados\n",
        "\n",
        "### Reto 87. Clasificación de ciudades por calidad de vida\n",
        "- Carga datos desde Numbeo API: `url = \"https://www.numbeo.com/api/rankings_by_city_current?section=1\"`\n",
        "- Convierte a DataFrame: `df = pd.json_normalize(data)`\n",
        "- Crea columna 'quality_tier' basada en `quality_of_life_index`:\n",
        "  - Si índice < 100: 'Baja calidad'\n",
        "  - Si entre 100-150: 'Calidad media'\n",
        "  - Si > 150: 'Alta calidad'\n",
        "- Usa `pd.cut()` para la clasificación\n",
        "- Cuenta ciudades por nivel de calidad e identifica el país con más ciudades en 'Alta calidad'\n",
        "\n",
        "### Reto 88. Window Functions: Ranking\n",
        "- Carga: `sns.load_dataset('diamonds')`\n",
        "- En diamonds, ordena por price descendente\n",
        "- Crea columna 'price_rank' con ranking: `df['price_rank'] = df['price'].rank(ascending=False)`\n",
        "- Identifica los 10 diamantes más caros\n",
        "- Calcula percentil de precio de cada diamante\n",
        "\n",
        "### Reto 89. Análisis de Criptomonedas con Media Móvil\n",
        "- Carga desde API:\n",
        "```python\n",
        "import requests\n",
        "url = 'https://api.coingecko.com/api/v3/coins/bitcoin/market_chart?vs_currency=usd&days=30'\n",
        "data = requests.get(url).json()\n",
        "df = pd.DataFrame(data['prices'], columns=['timestamp', 'price'])\n",
        "df['date'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "```\n",
        "- Ordena por fecha: `df.sort_values('date', inplace=True)`\n",
        "- Calcula media móvil de 7 días: `df['MA_7'] = df['price'].rolling(7).mean()`\n",
        "- Calcula media móvil de 14 días\n",
        "- Crea gráfico de líneas de precio con medias móviles superpuestas\n",
        "- Compara volatilidad con y sin suavizado\n",
        "\n",
        "### Reto 90. CTEs simulados con variables\n",
        "- Simula CTE (Common Table Expression):\n",
        "```python\n",
        "# CTE: pasajeros que sobrevivieron\n",
        "cte_survivors = titanic[titanic['survived']==1]\n",
        "# Query principal: edad promedio de sobrevivientes\n",
        "avg_age = cte_survivors['age'].mean()\n",
        "```\n",
        "- Crea CTE de pasajeros de primera clase\n",
        "- Calcula tarifa promedio de este grupo\n",
        "- Compara con tarifa promedio general\n",
        "\n",
        "### Reto 91. UNION simulado\n",
        "- Carga el dataset: `sns.load_dataset('tips')`\n",
        "- En tips, filtra meals de Saturday: `sat = df[df['day']=='Sat']`\n",
        "- Filtra meals de Sunday: `sun = df[df['day']=='Sun']`\n",
        "- Une ambos con concat: `pd.concat([sat, sun])`\n",
        "- Elimina duplicados si los hay\n",
        "- Verifica tamaño final\n",
        "\n",
        "### Reto 92. Pipeline ETL básico - Extract\n",
        "- Simula extracción de múltiples fuentes:\n",
        "```python\n",
        "# Fuente 1: CSV local\n",
        "sales_q1 = pd.read_csv('sales_q1.csv')\n",
        "# Fuente 2: JSON\n",
        "sales_q2 = pd.read_json('sales_q2.json')\n",
        "# Fuente 3: Excel\n",
        "sales_q3 = pd.read_excel('sales_q3.xlsx')\n",
        "```\n",
        "- Para práctica, crea estos archivos artificialmente primero\n",
        "- Verifica que todos se carguen correctamente\n",
        "\n",
        "### Reto 93. Pipeline ETL - Transform\n",
        "- Con los 3 DataFrames del Reto 92:\n",
        "- Estandariza nombres de columnas a minúsculas\n",
        "- Convierte fechas a formato datetime\n",
        "- Elimina duplicados en cada uno\n",
        "- Crea columna 'quarter' en cada DataFrame\n",
        "\n",
        "### Reto 94. Pipeline ETL - Load\n",
        "- Concatena los 3 DataFrames transformados verticalmente\n",
        "- Ordena por fecha\n",
        "- Valida que no haya nulos en columnas críticas\n",
        "- Exporta resultado consolidado a CSV\n",
        "- Crea log simple: `print(f\"Filas procesadas: {len(df)}, Fecha: {datetime.now()}\")`\n",
        "\n",
        "### Reto 95. Validación de calidad de datos\n",
        "- Carga el dataset: `sns.load_dataset('tips')`\n",
        "- Crea validaciones:\n",
        "```python\n",
        "# Check 1: No nulos en columnas críticas\n",
        "assert df['total_bill'].notna().all(), \"Hay nulos en total_bill\"\n",
        "# Check 2: Valores positivos\n",
        "assert (df['total_bill'] > 0).all(), \"Hay valores negativos\"\n",
        "# Check 3: Tip no mayor que bill\n",
        "assert (df['tip'] <= df['total_bill']).all(), \"Propina mayor que cuenta\"\n",
        "```\n",
        "- Añade check de rango de valores\n",
        "- Crea función de validación reutilizable\n",
        "\n",
        "---\n",
        "\n",
        "## **BLOQUE 7: ANÁLISIS AVANZADO Y DATOS COMPLEJOS (Retos 96-110)**\n",
        "\n",
        "### Reto 96. Detección de outliers con IQR\n",
        "- Carga: `sns.load_dataset('diamonds')`\n",
        "- En diamonds, calcula Q1, Q3 e IQR de carat\n",
        "- Define límites: `lower = Q1 - 1.5*IQR`, `upper = Q3 + 1.5*IQR`\n",
        "- Identifica outliers: `outliers = df[(df['carat'] < lower) | (df['carat'] > upper)]`\n",
        "- Calcula porcentaje de outliers\n",
        "- Visualiza con boxplot marcando outliers\n",
        "\n",
        "### Reto 97. Detección de outliers con Z-score\n",
        "- Carga: `sns.load_dataset('penguins')`\n",
        "- En penguins, calcula Z-score de body_mass_g\n",
        "- Identifica outliers (|Z| > 3)\n",
        "- Compara cantidad de outliers con método IQR\n",
        "- ¿Qué método es más estricto?\n",
        "\n",
        "### Reto 98. Análisis de outliers multivariante\n",
        "- Carga Iris desde Seaborn: `sns.load_dataset('iris')`\n",
        "- En iris, identifica outliers combinando:\n",
        "- petal_length > Q3 + 1.5*IQR\n",
        "- sepal_width < Q1 - 1.5*IQR\n",
        "- Usa operador & para condición compuesta\n",
        "- Visualiza outliers en scatterplot con color diferente\n",
        "\n",
        "### Reto 99. Imputación de valores nulos - Media/Mediana\n",
        "- Carga: `sns.load_dataset('penguins')`\n",
        "- Carga penguins (con nulos)\n",
        "- Crea 3 copias del dataset\n",
        "- Versión 1: elimina nulos con dropna()\n",
        "- Versión 2: imputa con media\n",
        "- Versión 3: imputa con mediana\n",
        "- Compara estadísticas descriptivas de las 3 versiones\n",
        "\n",
        "### Reto 100. Imputación de valores nulos - Forward Fill\n",
        "- Carga: `sns.load_dataset('flights')`\n",
        "- En series temporal de flights, introduce nulos artificiales:\n",
        "```python\n",
        "df_missing = df.copy()\n",
        "df_missing.loc[10:15, 'passengers'] = np.nan\n",
        "```\n",
        "- Aplica forward fill: `df_missing['passengers'].fillna(method='ffill')`\n",
        "- Aplica backward fill: `fillna(method='bfill')`\n",
        "- Compara ambos métodos visualmente\n",
        "\n",
        "### Reto 101. Análisis de sesgo y curtosis\n",
        "- Carga: `sns.load_dataset('diamonds')`\n",
        "```python\n",
        "from scipy.stats import skew, kurtosis\n",
        "```\n",
        "- Calcula sesgo de price en diamonds\n",
        "- Calcula curtosis de la misma variable\n",
        "- Interpreta: sesgo > 0 (asimetría derecha), curtosis > 3 (leptocúrtica)\n",
        "- Compara con distribución normal en histograma\n",
        "\n",
        "### Reto 102. Transformación logarítmica\n",
        "- Carga: `sns.load_dataset('diamonds')`\n",
        "- En diamonds, price está muy sesgado\n",
        "- Aplica transformación log: `df['log_price'] = np.log(df['price'])`\n",
        "- Compara histogramas de price vs log_price\n",
        "- Calcula nuevo sesgo de log_price\n",
        "- ¿La transformación normalizó la distribución?\n",
        "\n",
        "### Reto 103. Transformación Box-Cox\n",
        "- Carga: `sns.load_dataset('diamonds')`\n",
        "```python\n",
        "from scipy.stats import boxcox\n",
        "```\n",
        "- Aplica transformación Box-Cox a price en diamonds:\n",
        "```python\n",
        "transformed, lambda_param = boxcox(df['price'])\n",
        "df['boxcox_price'] = transformed\n",
        "```\n",
        "- Compara distribución original vs transformada\n",
        "- Box-Cox encuentra automáticamente mejor transformación\n",
        "\n",
        "### Reto 104. Binning de variables continuas\n",
        "- Carga Titanic desde Seaborn: `sns.load_dataset('titanic')`\n",
        "- En titanic, crea bins de edad:\n",
        "```python\n",
        "bins = [0, 12, 18, 35, 60, 100]\n",
        "labels = ['Niño', 'Adolescente', 'Adulto Joven', 'Adulto', 'Senior']\n",
        "df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels)\n",
        "```\n",
        "- Calcula tasa de supervivencia por grupo de edad\n",
        "- Visualiza con gráfico de barras\n",
        "\n",
        "### Reto 105. Encoding de variables categóricas - Label Encoding\n",
        "- Carga: `sns.load_dataset('penguins')`\n",
        "- En penguins, convierte species a números:\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['species_encoded'] = le.fit_transform(df['species'])\n",
        "```\n",
        "- Verifica mapeo: Adelie=0, Chinstrap=1, Gentoo=2\n",
        "- Guarda mapeo para referencia futura\n",
        "\n",
        "### Reto 106. Encoding de variables categóricas - One-Hot Encoding\n",
        "- Carga: `sns.load_dataset('penguins')`\n",
        "- En penguins, aplica one-hot encoding a island:\n",
        "```python\n",
        "island_dummies = pd.get_dummies(df['island'], prefix='island')\n",
        "df = pd.concat([df, island_dummies], axis=1)\n",
        "```\n",
        "- Verifica que se crearon 3 columnas binarias\n",
        "- ¿Cuándo usar Label vs One-Hot encoding?\n",
        "\n",
        "### Reto 107. Feature Engineering - Interacciones\n",
        "- Carga el dataset: `sns.load_dataset('tips')`\n",
        "- En tips, crea features de interacción:\n",
        "```python\n",
        "df['bill_per_person'] = df['total_bill'] / df['size']\n",
        "df['tip_per_person'] = df['tip'] / df['size']\n",
        "df['tip_rate'] = df['tip'] / df['total_bill']\n",
        "```\n",
        "- Calcula correlación de nuevas features con tip\n",
        "- ¿Alguna feature nueva es más predictiva?\n",
        "\n",
        "### Reto 108. Feature Engineering - Fechas\n",
        "- Carga: `sns.load_dataset('flights')`\n",
        "- En flights, crea features temporales:\n",
        "```python\n",
        "df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
        "df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
        "```\n",
        "- Esto captura ciclicidad de meses\n",
        "- Visualiza relación entre estas features y passengers\n",
        "\n",
        "### Reto 109. Análisis de componentes principales (PCA) - Preparación\n",
        "- Carga Iris desde Seaborn: `sns.load_dataset('iris')`\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "```\n",
        "- Carga Iris desde Seaborn: `sns.load_dataset('iris')`\n",
        "- Selecciona solo columnas numéricas\n",
        "- Estandariza datos: `scaler = StandardScaler()` y `scaled = scaler.fit_transform(df_numeric)`\n",
        "- Verifica que media ≈ 0 y std ≈ 1\n",
        "- Prepara para PCA (siguiente reto)\n",
        "\n",
        "### Reto 110. Análisis de componentes principales (PCA)\n",
        "- Carga Iris desde Seaborn: `sns.load_dataset('iris')`\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "```\n",
        "- Aplica PCA con 2 componentes: `pca = PCA(n_components=2)`\n",
        "- Transforma datos: `transformed = pca.fit_transform(scaled)`\n",
        "- Visualiza datos en 2D coloreados por especie\n",
        "- Imprime varianza explicada: `pca.explained_variance_ratio_`\n",
        "\n",
        "---\n",
        "\n",
        "## **BLOQUE 8: VISUALIZACIÓN AVANZADA (Retos 111-120)**\n",
        "\n",
        "### Reto 111. Subplots múltiples\n",
        "- Carga Iris desde Seaborn: `sns.load_dataset('iris')`\n",
        "- Crea figura con 4 subplots (2x2):\n",
        "```python\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "```\n",
        "- Plot 1: Histograma de sepal_length en iris\n",
        "- Plot 2: Scatterplot de sepal_length vs petal_length\n",
        "- Plot 3: Boxplot de petal_width por especie\n",
        "- Plot 4: Heatmap de correlaciones\n",
        "- Añade título general a la figura\n",
        "\n",
        "### Reto 112. Gráficos de distribución comparativos\n",
        "- Carga: `sns.load_dataset('penguins')`\n",
        "- En penguins, crea figura con 3 distribuciones:\n",
        "- Histograma de body_mass_g\n",
        "- KDE (Kernel Density Estimate) de body_mass_g\n",
        "- Histograma + KDE superpuestos\n",
        "- Usa `sns.histplot()` con `kde=True`\n",
        "\n",
        "### Reto 113. FacetGrid de Seaborn\n",
        "- Carga el dataset: `sns.load_dataset('tips')`\n",
        "- En tips, crea FacetGrid por time y smoker:\n",
        "```python\n",
        "g = sns.FacetGrid(df, col='time', row='smoker', height=4)\n",
        "g.map(plt.hist, 'total_bill')\n",
        "```\n",
        "- Añade títulos descriptivos\n",
        "- Analiza diferencias entre grupos\n",
        "\n",
        "### Reto 114. Pairplot avanzado con personalización\n",
        "- Carga: `sns.load_dataset('penguins')`\n",
        "- En penguins, crea pairplot con:\n",
        "- hue='species'\n",
        "- diag_kind='kde'\n",
        "- markers=['o', 's', 'D'] para cada especie\n",
        "- Personaliza paleta de colores\n",
        "- Añade título general\n",
        "\n",
        "### Reto 115. Gráfico de barras apiladas\n",
        "- Carga Titanic desde Seaborn: `sns.load_dataset('titanic')`\n",
        "- En titanic, crea tabla cruzada de class vs sex\n",
        "- Crea gráfico de barras apiladas:\n",
        "```python\n",
        "cross_tab = pd.crosstab(df['class'], df['sex'])\n",
        "cross_tab.plot(kind='bar', stacked=True)\n",
        "```\n",
        "- Añade leyenda, título y etiquetas\n",
        "- Analiza composición de género por clase\n",
        "\n",
        "### Reto 116. Gráfico de áreas apiladas\n",
        "- Carga: `sns.load_dataset('flights')`\n",
        "- En flights, agrupa por year y calcula total passengers\n",
        "- Simula diferentes categorías (crea columnas artificiales)\n",
        "- Crea gráfico de áreas apiladas:\n",
        "```python\n",
        "df.plot(kind='area', stacked=True)\n",
        "```\n",
        "- Útil para mostrar composición a lo largo del tiempo\n",
        "\n",
        "### Reto 117. Heatmap con anotaciones personalizadas\n",
        "- Carga: `sns.load_dataset('flights')`\n",
        "- En flights, crea pivot table de passengers por year-month\n",
        "- Crea heatmap con:\n",
        "```python\n",
        "sns.heatmap(pivot, annot=True, fmt='d', cmap='YlGnBu',\n",
        "            linewidths=0.5, linecolor='gray')\n",
        "```\n",
        "- Personaliza colorbar\n",
        "- Añade título descriptivo\n",
        "\n",
        "### Reto 118. Gráfico de regresión con intervalo de confianza\n",
        "- Carga el dataset: `sns.load_dataset('tips')`\n",
        "- En tips, crea regplot de total_bill vs tip:\n",
        "```python\n",
        "sns.regplot(x='total_bill', y='tip', data=df,\n",
        "            scatter_kws={'alpha':0.5},\n",
        "            line_kws={'color':'red'})\n",
        "```\n",
        "- Muestra banda de confianza\n",
        "- Añade ecuación de regresión en título\n",
        "\n",
        "### Reto 119. Jointplot para análisis bivariado\n",
        "- Carga: `sns.load_dataset('penguins')`\n",
        "- En penguins, crea jointplot de flipper_length vs body_mass:\n",
        "```python\n",
        "sns.jointplot(x='flipper_length_mm', y='body_mass_g',\n",
        "              data=df, kind='scatter', hue='species')\n",
        "```\n",
        "- Prueba diferentes kinds: 'hex', 'kde', 'reg'\n",
        "- Analiza distribuciones marginales\n",
        "\n",
        "### Reto 120. Dashboard visual completo - Proyecto Integrador Final\n",
        "- Carga: `sns.load_dataset('gapminder')`\n",
        "- Usa gapminder del año 2007\n",
        "- Crea figura con 6 subplots (2x3):\n",
        "\n",
        "**Plot 1:** Scatterplot de gdpPercap vs lifeExp, tamaño por pop, color por continent\n",
        "**Plot 2:** Gráfico de barras horizontales de lifeExp promedio por continente (ordenado)\n",
        "**Plot 3:** Boxplot de gdpPercap por continente\n",
        "**Plot 4:** Histograma de distribución de lifeExp con KDE\n",
        "**Plot 5:** Heatmap de correlación entre variables numéricas\n",
        "**Plot 6:** Top 10 países por gdpPercap (barras horizontales)\n",
        "\n",
        "- Añade título general: \"Análisis Global de Desarrollo 2007\"\n",
        "- Personaliza colores, etiquetas y leyendas\n",
        "- Guarda como imagen PNG de alta calidad: `plt.savefig('dashboard_final.png', dpi=300, bbox_inches='tight')`\n",
        "- **Reflexión final:** Escribe en comentarios 3 insights principales del análisis\n",
        "\n",
        "---\n",
        "\n",
        "# RESUMEN DE LOS 120 RETOS\n",
        "\n",
        "## Distribución por Bloques\n",
        "\n",
        "| Bloque | Retos | Enfoque Principal |\n",
        "|--------|-------|-------------------|\n",
        "| **1. Fundamentos y Carga** | 1-10 | Carga de datos, exploración básica, primeras visualizaciones |\n",
        "| **2. EDA y Visualización** | 11-25 | Análisis exploratorio, estadísticas, gráficos básicos |\n",
        "| **3. Operaciones y Transformaciones** | 26-45 | Manipulación de datos, agregaciones, transformaciones |\n",
        "| **4. Series Temporales** | 46-60 | Datos temporales, tendencias, análisis histórico |\n",
        "| **5. Datos del Mundo Real** | 61-80 | APIs, web scraping, datasets externos, JSON |\n",
        "| **6. Ingeniería de Datos** | 81-95 | SQL simulado, ETL, validación, pipelines |\n",
        "| **7. Análisis Avanzado** | 96-110 | Outliers, imputación, transformaciones, PCA |\n",
        "| **8. Visualización Avanzada** | 111-120 | Gráficos complejos, dashboard, proyecto final |\n",
        "\n",
        "## Fuentes de Datos Utilizadas (Públicas y Accesibles)\n",
        "\n",
        "### Datasets de Seaborn (16 datasets)\n",
        "- titanic, iris, tips, penguins, flights, diamonds, mpg, planets\n",
        "- anscombe, car_crashes, gapminder\n",
        "\n",
        "### Datasets de scikit-learn (2 datasets)\n",
        "- breast_cancer, (california_housing removido por complejidad)\n",
        "\n",
        "### URLs Públicas Verificadas (10+ datasets)\n",
        "- Global Temperature (Berkeley Earth)\n",
        "- COVID-19 (Our World in Data)\n",
        "- Wine Quality (UCI)\n",
        "- Student Performance (UCI)\n",
        "- Country Data (GitHub JSON)\n",
        "- Earthquakes (USGS)\n",
        "- Currency Exchange (open.er-api.com)\n",
        "- Weather Forecast (Open-Meteo)\n",
        "- Wikipedia Tables (web scraping)\n",
        "- Public Holidays (Nager.Date API)\n",
        "- Universities (GitHub JSON)\n",
        "\n",
        "### APIs Públicas Sin Autenticación (5 APIs)\n",
        "- Open-Meteo (clima)\n",
        "- USGS Earthquakes (terremotos)\n",
        "- Exchange Rates API (divisas)\n",
        "- Nager.Date (feriados)\n",
        "- Open Data portals (calidad del aire)\n",
        "\n",
        "## Cobertura del Temario\n",
        "\n",
        "| Área del Temario | Retos que la Cubren | Cobertura |\n",
        "|------------------|---------------------|-----------|\n",
        "| **Bloque 1: Ecosistema de Datos** | 1-10, 61-80 | ⭐⭐⭐⭐⭐ |\n",
        "| **Bloque 2: EDA y Visualización** | 11-45, 111-120 | ⭐⭐⭐⭐⭐ |\n",
        "| **Bloque 3: Estadística y Ciencia de Datos** | 46-60, 96-110 | ⭐⭐⭐⭐⭐ |\n",
        "| **Bloque 4: Ingeniería de Datos** | 81-95 | ⭐⭐⭐⭐⭐ |\n",
        "| **Bloque 5: Proyecto Integrador** | 120 | ⭐⭐⭐⭐⭐ |\n",
        "\n",
        "## Características Técnicas\n",
        "\n",
        "- **Total de retos:** 120\n",
        "- **Datasets únicos:** 35+\n",
        "- **APIs públicas:** 5\n",
        "- **Lenguajes/Herramientas:** Python, Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn, SciPy\n",
        "- **Formatos de datos:** CSV, JSON, Excel, HTML, APIs REST\n",
        "- **Todos los recursos:** Públicos, gratuitos y verificados\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "TyCRRGZG6pAv"
      }
    }
  ]
}