{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAzlZrBACtikmZl7BeSxWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/financieras/big_data/blob/main/leccion_1_1_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecci√≥n 1.1.7: El ciclo de vida de los datos en la organizaci√≥n\n",
        "\n",
        "## 1. Introducci√≥n: Los datos como activo vivo\n",
        "\n",
        "Los datos no son un archivo est√°tico que guardamos y olvidamos. Son un **activo vivo** que fluye continuamente a trav√©s de la organizaci√≥n, transform√°ndose en cada etapa, generando valor y eventualmente envejeciendo.\n",
        "\n",
        "**Lo importante:** Comprender el ciclo de vida completo de los datos es fundamental para dise√±ar arquitecturas eficientes, garantizar calidad, cumplir regulaciones y extraer el m√°ximo valor. Cada etapa tiene sus propios desaf√≠os, responsabilidades y herramientas.\n",
        "\n",
        "### La analog√≠a del agua en una ciudad\n",
        "\n",
        "Piensa en los datos como el agua en una ciudad:\n",
        "\n",
        "1. **Generaci√≥n:** Lluvia y manantiales (fuentes de datos)\n",
        "2. **Recolecci√≥n:** Captaci√≥n en embalses (ingesta)\n",
        "3. **Tratamiento:** Planta potabilizadora (limpieza y transformaci√≥n)\n",
        "4. **Distribuci√≥n:** Red de tuber√≠as (almacenamiento y acceso)\n",
        "5. **Consumo:** Hogares y empresas (an√°lisis y decisiones)\n",
        "6. **Reciclaje:** Tratamiento de aguas residuales (archivo)\n",
        "7. **Eliminaci√≥n:** Desechos no reutilizables (borrado por compliance)\n",
        "\n",
        "Al igual que el agua, los datos necesitan **infraestructura, calidad, gobernanza y gesti√≥n del ciclo completo**.\n",
        "\n",
        "### ¬øPor qu√© importa el ciclo de vida?\n",
        "\n",
        "**Problemas comunes de ignorar el ciclo de vida:**\n",
        "- üí∞ Costes descontrolados (almacenar terabytes de datos que nunca se usan)\n",
        "- ‚öñÔ∏è Incumplimiento legal (retener datos m√°s all√° de lo permitido por GDPR)\n",
        "- üêõ Baja calidad (datos obsoletos o incorrectos en an√°lisis)\n",
        "- üîí Riesgos de seguridad (informaci√≥n sensible no protegida)\n",
        "- ‚è±Ô∏è Decisiones lentas (no encontrar los datos cuando se necesitan)\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Las 7 etapas del ciclo de vida de los datos\n",
        "\n",
        "### Visi√≥n general\n",
        "\n",
        "```\n",
        "1. GENERACI√ìN ‚Üí 2. RECOLECCI√ìN ‚Üí 3. ALMACENAMIENTO ‚Üí 4. PROCESAMIENTO\n",
        "                                                              ‚Üì\n",
        "                                            7. ELIMINACI√ìN ‚Üê 6. ARCHIVO ‚Üê 5. AN√ÅLISIS/USO\n",
        "```\n",
        "\n",
        "| Etapa | Pregunta clave | Responsable principal | Duraci√≥n |\n",
        "|-------|----------------|----------------------|----------|\n",
        "| 1. Generaci√≥n | ¬øDe d√≥nde vienen los datos? | Sistemas fuente, usuarios | Continua |\n",
        "| 2. Recolecci√≥n | ¬øC√≥mo los capturamos? | Data Engineer | Segundos-horas |\n",
        "| 3. Almacenamiento | ¬øD√≥nde los guardamos? | Data Engineer | D√≠as-a√±os |\n",
        "| 4. Procesamiento | ¬øC√≥mo los transformamos? | Data Engineer | Minutos-horas |\n",
        "| 5. An√°lisis/Uso | ¬øQu√© valor extraemos? | Analyst, Scientist | Variable |\n",
        "| 6. Archivo | ¬øC√≥mo preservamos hist√≥ricos? | Data Engineer, Architect | A√±os |\n",
        "| 7. Eliminaci√≥n | ¬øCu√°ndo los borramos? | Data Engineer, Legal | Seg√∫n pol√≠tica |\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Etapa 1: Generaci√≥n de datos\n",
        "\n",
        "### Definici√≥n\n",
        "\n",
        "La **generaci√≥n** es el momento en que los datos nacen. Es el origen de todo el ciclo. Los datos pueden generarse de m√∫ltiples formas y fuentes.\n",
        "\n",
        "### Tipos de generaci√≥n\n",
        "\n",
        "**1. Datos generados por humanos:**\n",
        "- Formularios web (registros, compras)\n",
        "- Emails y mensajes\n",
        "- Posts en redes sociales\n",
        "- Documentos y spreadsheets\n",
        "- Clicks y navegaci√≥n web\n",
        "\n",
        "**2. Datos generados por m√°quinas:**\n",
        "- Logs de servidores\n",
        "- Sensores IoT (temperatura, presi√≥n, ubicaci√≥n)\n",
        "- Transacciones autom√°ticas\n",
        "- Telemetr√≠a de aplicaciones\n",
        "- Datos de sistemas industriales\n",
        "\n",
        "**3. Datos derivados:**\n",
        "- Agregaciones de otros datos\n",
        "- Features calculadas para ML\n",
        "- M√©tricas y KPIs\n",
        "- Predicciones de modelos\n",
        "\n",
        "### Ejemplo: Generaci√≥n de datos en Uber\n",
        "\n",
        "**Una solicitud de viaje genera:**\n",
        "\n",
        "```\n",
        "Momento de solicitud:\n",
        "‚îú‚îÄ Datos del usuario: user_id, ubicaci√≥n GPS, rating\n",
        "‚îú‚îÄ Datos de la solicitud: timestamp, destino, tipo de veh√≠culo\n",
        "‚îú‚îÄ Datos de contexto: clima, tr√°fico, eventos cercanos\n",
        "‚îú‚îÄ Datos de precios: estimaci√≥n inicial, surge pricing\n",
        "\n",
        "Durante el viaje:\n",
        "‚îú‚îÄ GPS tracking: ubicaci√≥n cada 5 segundos\n",
        "‚îú‚îÄ Telemetr√≠a del conductor: velocidad, aceleraci√≥n, frenadas\n",
        "‚îú‚îÄ Datos del veh√≠culo: consumo, temperatura\n",
        "\n",
        "Al finalizar:\n",
        "‚îú‚îÄ Datos de pago: m√©todo, monto, propina\n",
        "‚îú‚îÄ Ratings: calificaci√≥n conductor/pasajero\n",
        "‚îú‚îÄ Datos de viaje: duraci√≥n real, ruta tomada, distancia\n",
        "```\n",
        "\n",
        "**Volumen:** Un viaje promedio genera ~5-10MB de datos. Con 15 millones de viajes diarios = 75-150 TB/d√≠a solo de viajes.\n",
        "\n",
        "### Caracter√≠sticas de los datos en generaci√≥n\n",
        "\n",
        "| Aspecto | Descripci√≥n |\n",
        "|---------|-------------|\n",
        "| **Velocidad** | Streaming (continua) vs Batch (peri√≥dica) |\n",
        "| **Volumen** | Variable seg√∫n fuente (bytes a terabytes) |\n",
        "| **Estructura** | Estructurados, semi-estructurados, no estructurados |\n",
        "| **Calidad** | Frecuentemente imperfecta (campos vac√≠os, errores) |\n",
        "| **Propiedad** | ¬øQui√©n es due√±o de estos datos? |\n",
        "\n",
        "### Consideraciones clave\n",
        "\n",
        "**Momento de generaci√≥n = Momento cr√≠tico para calidad**\n",
        "\n",
        "‚ùå **Error com√∫n:** \"Ya limpiaremos los datos despu√©s\"\n",
        "\n",
        "‚úÖ **Best practice:** Validaci√≥n en origen (validation at source)\n",
        "\n",
        "**Ejemplo de validaci√≥n en origen:**\n",
        "```python\n",
        "from pydantic import BaseModel, validator, EmailStr\n",
        "from datetime import datetime\n",
        "\n",
        "class UserRegistration(BaseModel):\n",
        "    email: EmailStr  # Valida formato de email\n",
        "    age: int\n",
        "    country_code: str\n",
        "    timestamp: datetime\n",
        "    \n",
        "    @validator('age')\n",
        "    def age_must_be_reasonable(cls, v):\n",
        "        if v < 0 or v > 120:\n",
        "            raise ValueError('Age must be between 0 and 120')\n",
        "        return v\n",
        "    \n",
        "    @validator('country_code')\n",
        "    def country_code_valid(cls, v):\n",
        "        valid_codes = ['ES', 'FR', 'DE', 'IT', 'UK', 'US']\n",
        "        if v not in valid_codes:\n",
        "            raise ValueError(f'Invalid country code: {v}')\n",
        "        return v\n",
        "\n",
        "# Uso\n",
        "try:\n",
        "    user = UserRegistration(\n",
        "        email=\"user@example.com\",\n",
        "        age=25,\n",
        "        country_code=\"ES\",\n",
        "        timestamp=datetime.now()\n",
        "    )\n",
        "    # Datos validados, guardar en base de datos\n",
        "except ValueError as e:\n",
        "    # Rechazar datos inv√°lidos en el momento de generaci√≥n\n",
        "    log_error(f\"Invalid data: {e}\")\n",
        "```\n",
        "\n",
        "**Beneficio:** Prevenir entrada de datos incorrectos ahorra tiempo y dinero en limpieza posterior.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Etapa 2: Recolecci√≥n (Ingesta)\n",
        "\n",
        "### Definici√≥n\n",
        "\n",
        "La **recolecci√≥n o ingesta** es el proceso de capturar datos desde sus fuentes y moverlos hacia los sistemas de la organizaci√≥n para su posterior procesamiento.\n",
        "\n",
        "### M√©todos de ingesta\n",
        "\n",
        "**1. Batch ingestion (ingesta por lotes):**\n",
        "- Datos se recolectan en intervalos regulares (horario, diario)\n",
        "- M√°s simple y econ√≥mica\n",
        "- Latencia alta (horas/d√≠as)\n",
        "\n",
        "**Ejemplo:** Exportaci√≥n diaria de ventas desde un CRM\n",
        "\n",
        "```python\n",
        "# Airflow DAG para ingesta batch diaria\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def extract_salesforce_data():\n",
        "    # Conectar a API de Salesforce\n",
        "    sales_data = salesforce_api.get_opportunities(\n",
        "        start_date=datetime.now() - timedelta(days=1),\n",
        "        end_date=datetime.now()\n",
        "    )\n",
        "    # Guardar en S3\n",
        "    s3.upload(sales_data, f's3://raw-data/salesforce/{datetime.now().date()}.json')\n",
        "\n",
        "dag = DAG(\n",
        "    'daily_salesforce_ingestion',\n",
        "    schedule_interval='0 2 * * *',  # 2 AM diaria\n",
        "    start_date=datetime(2025, 1, 1)\n",
        ")\n",
        "\n",
        "task = PythonOperator(\n",
        "    task_id='extract_salesforce',\n",
        "    python_callable=extract_salesforce_data,\n",
        "    dag=dag\n",
        ")\n",
        "```\n",
        "\n",
        "**2. Real-time/Streaming ingestion (tiempo real):**\n",
        "- Datos capturados continuamente al generarse\n",
        "- Mayor complejidad y coste\n",
        "- Latencia baja (segundos/milisegundos)\n",
        "\n",
        "**Ejemplo:** Eventos de clicks en una aplicaci√≥n m√≥vil\n",
        "\n",
        "```python\n",
        "# Producer de Kafka para streaming\n",
        "from kafka import KafkaProducer\n",
        "import json\n",
        "\n",
        "producer = KafkaProducer(\n",
        "    bootstrap_servers=['kafka1:9092', 'kafka2:9092'],\n",
        "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
        ")\n",
        "\n",
        "def track_user_event(user_id, event_type, properties):\n",
        "    event = {\n",
        "        'user_id': user_id,\n",
        "        'event_type': event_type,\n",
        "        'properties': properties,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    # Enviar a Kafka inmediatamente\n",
        "    producer.send('user_events', value=event)\n",
        "    producer.flush()\n",
        "\n",
        "# Cada click se env√≠a instant√°neamente\n",
        "track_user_event(\n",
        "    user_id='user_123',\n",
        "    event_type='product_view',\n",
        "    properties={'product_id': 'prod_456', 'category': 'electronics'}\n",
        ")\n",
        "```\n",
        "\n",
        "**3. Micro-batch ingestion:**\n",
        "- H√≠brido: peque√±os lotes frecuentes (cada minuto)\n",
        "- Balance entre simplicidad y latencia\n",
        "- Popular con Spark Structured Streaming\n",
        "\n",
        "### Patrones de ingesta\n",
        "\n",
        "| Patr√≥n | Cu√°ndo usar | Tecnolog√≠a t√≠pica |\n",
        "|--------|-------------|------------------|\n",
        "| **Full load** | Primera carga, tablas peque√±as | Scripts SQL, ETL tools |\n",
        "| **Incremental** | Solo cambios desde √∫ltima carga | CDC (Change Data Capture) |\n",
        "| **Delta/Upsert** | Actualizaciones y nuevos registros | Merge statements, Delta Lake |\n",
        "| **Event streaming** | Eventos en tiempo real | Kafka, Kinesis, Pub/Sub |\n",
        "| **API polling** | SaaS tools (Salesforce, Stripe) | Fivetran, Airbyte, custom scripts |\n",
        "| **File drop** | Vendors externos | S3 event triggers, file watchers |\n",
        "\n",
        "### Ejemplo real: Ingesta en Spotify\n",
        "\n",
        "**Desaf√≠o:** Ingestar datos de reproducci√≥n de 500M+ usuarios activos.\n",
        "\n",
        "**Arquitectura de ingesta:**\n",
        "\n",
        "```\n",
        "Aplicaci√≥n m√≥vil/web\n",
        "    ‚Üì (eventos de reproducci√≥n)\n",
        "Kafka (100M eventos/hora)\n",
        "    ‚Üì\n",
        "Spark Streaming (procesamiento inicial)\n",
        "    ‚Üì\n",
        "S3 (data lake - formato Parquet)\n",
        "    ‚Üì\n",
        "Snowflake (data warehouse para an√°lisis)\n",
        "```\n",
        "\n",
        "**Datos generados:**\n",
        "- **Event:** Play, pause, skip, like, add to playlist\n",
        "- **Metadata:** Song ID, artist, album, timestamp, device, location\n",
        "- **Volumen:** ~10 TB/d√≠a\n",
        "\n",
        "**Decisiones t√©cnicas:**\n",
        "- Streaming para eventos de reproducci√≥n (necesitan actualizarse constantemente)\n",
        "- Batch para metadata de canciones (cambia raramente)\n",
        "- Particionamiento por fecha y regi√≥n para optimizar queries\n",
        "\n",
        "### Desaf√≠os comunes en recolecci√≥n\n",
        "\n",
        "**1. Manejo de fallos:**\n",
        "\n",
        "‚ùå **Mal enfoque:** Si falla, se pierden datos\n",
        "\n",
        "‚úÖ **Buen enfoque:** Retry logic + dead letter queues\n",
        "\n",
        "```python\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=1, min=4, max=10)\n",
        ")\n",
        "def ingest_from_api():\n",
        "    try:\n",
        "        data = api.fetch_data()\n",
        "        store_in_database(data)\n",
        "    except Exception as e:\n",
        "        # Si falla despu√©s de 3 intentos, enviar a dead letter queue\n",
        "        dead_letter_queue.send(data)\n",
        "        raise\n",
        "```\n",
        "\n",
        "**2. Idempotencia:**\n",
        "- Ejecutar la ingesta m√∫ltiples veces debe dar el mismo resultado\n",
        "- Cr√≠tico para reprocessing\n",
        "\n",
        "**3. Schema evolution:**\n",
        "- Las fuentes de datos cambian con el tiempo\n",
        "- Necesitas manejar campos nuevos, eliminados o modificados\n",
        "\n",
        "**4. Rate limits:**\n",
        "- APIs externas tienen l√≠mites de requests\n",
        "- Implementar throttling y backoff\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Etapa 3: Almacenamiento\n",
        "\n",
        "### Definici√≥n\n",
        "\n",
        "El **almacenamiento** es donde los datos residen despu√©s de ser recolectados, esperando ser procesados o consultados.\n",
        "\n",
        "### Tipos de almacenamiento por prop√≥sito\n",
        "\n",
        "**1. Transactional databases (OLTP):**\n",
        "- **Prop√≥sito:** Operaciones diarias del negocio\n",
        "- **Caracter√≠sticas:** ACID, low latency, muchas escrituras peque√±as\n",
        "- **Tecnolog√≠as:** PostgreSQL, MySQL, MongoDB\n",
        "- **Ejemplo:** Base de datos de pedidos de e-commerce\n",
        "\n",
        "**2. Data warehouses (OLAP):**\n",
        "- **Prop√≥sito:** An√°lisis y reporting\n",
        "- **Caracter√≠sticas:** Optimizado para lecturas, consultas complejas, datos hist√≥ricos\n",
        "- **Tecnolog√≠as:** Snowflake, BigQuery, Redshift\n",
        "- **Ejemplo:** Warehouse de ventas hist√≥ricas para an√°lisis de tendencias\n",
        "\n",
        "**3. Data lakes:**\n",
        "- **Prop√≥sito:** Almacenamiento masivo de datos raw\n",
        "- **Caracter√≠sticas:** Schema-on-read, cualquier formato, bajo coste\n",
        "- **Tecnolog√≠as:** S3, Azure Data Lake Storage, Google Cloud Storage\n",
        "- **Ejemplo:** Logs de aplicaci√≥n, im√°genes, datos no estructurados\n",
        "\n",
        "**4. Data lakehouses:**\n",
        "- **Prop√≥sito:** Unificar lake + warehouse\n",
        "- **Caracter√≠sticas:** Flexibilidad del lake + performance del warehouse\n",
        "- **Tecnolog√≠as:** Databricks (Delta Lake), Snowflake, Iceberg\n",
        "- **Ejemplo:** Plataforma unificada para analytics y ML\n",
        "\n",
        "### Arquitectura medallion (Lakehouse)\n",
        "\n",
        "Patr√≥n moderno para organizar datos en capas:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  BRONZE (Raw data)                      ‚îÇ\n",
        "‚îÇ  - Datos tal cual llegan                ‚îÇ\n",
        "‚îÇ  - Sin transformaciones                 ‚îÇ\n",
        "‚îÇ  - Schema m√≠nimo o inexistente          ‚îÇ\n",
        "‚îÇ  - Retenci√≥n: 30-90 d√≠as                ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "            ‚Üì (limpieza b√°sica)\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  SILVER (Cleaned data)                  ‚îÇ\n",
        "‚îÇ  - Datos validados y limpios            ‚îÇ\n",
        "‚îÇ  - Schema definido                      ‚îÇ\n",
        "‚îÇ  - Duplicados eliminados                ‚îÇ\n",
        "‚îÇ  - Retenci√≥n: 1-2 a√±os                  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "            ‚Üì (agregaciones)\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  GOLD (Business-level aggregates)       ‚îÇ\n",
        "‚îÇ  - Datos listos para negocio            ‚îÇ\n",
        "‚îÇ  - KPIs, m√©tricas, features para ML     ‚îÇ\n",
        "‚îÇ  - Altamente optimizados                ‚îÇ\n",
        "‚îÇ  - Retenci√≥n: Indefinida                ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "**Ejemplo en c√≥digo (dbt):**\n",
        "\n",
        "```sql\n",
        "-- bronze/raw_orders.sql\n",
        "-- Simplemente copia de la fuente\n",
        "SELECT * FROM source_db.orders\n",
        "\n",
        "-- silver/clean_orders.sql\n",
        "-- Limpieza y validaci√≥n\n",
        "SELECT\n",
        "    order_id,\n",
        "    customer_id,\n",
        "    order_date,\n",
        "    CAST(amount AS DECIMAL(10,2)) as amount,\n",
        "    status\n",
        "FROM {{ ref('raw_orders') }}\n",
        "WHERE\n",
        "    amount > 0\n",
        "    AND customer_id IS NOT NULL\n",
        "    AND order_date >= '2020-01-01'\n",
        "\n",
        "-- gold/daily_revenue.sql\n",
        "-- Agregaci√≥n para negocio\n",
        "SELECT\n",
        "    DATE(order_date) as date,\n",
        "    COUNT(DISTINCT order_id) as orders,\n",
        "    COUNT(DISTINCT customer_id) as customers,\n",
        "    SUM(amount) as revenue,\n",
        "    AVG(amount) as avg_order_value\n",
        "FROM {{ ref('clean_orders') }}\n",
        "WHERE status = 'completed'\n",
        "GROUP BY DATE(order_date)\n",
        "```\n",
        "\n",
        "### Hot, warm, cold data\n",
        "\n",
        "Estrategia de almacenamiento seg√∫n frecuencia de acceso:\n",
        "\n",
        "| Tier | Descripci√≥n | Tiempo de acceso | Coste | Ejemplo |\n",
        "|------|-------------|------------------|-------|---------|\n",
        "| **Hot** | Acceso frecuente | Milisegundos | Alto | Datos √∫ltimos 7 d√≠as |\n",
        "| **Warm** | Acceso ocasional | Segundos | Medio | Datos 7-90 d√≠as |\n",
        "| **Cold** | Acceso raro | Minutos-horas | Bajo | Datos >90 d√≠as |\n",
        "| **Archive** | Casi nunca | Horas | Muy bajo | Datos >1 a√±o, compliance |\n",
        "\n",
        "**Ejemplo de pol√≠ticas en AWS S3:**\n",
        "\n",
        "```python\n",
        "# Lifecycle policy para S3\n",
        "lifecycle_policy = {\n",
        "    'Rules': [\n",
        "        {\n",
        "            'Id': 'Move to Glacier',\n",
        "            'Status': 'Enabled',\n",
        "            'Transitions': [\n",
        "                {\n",
        "                    'Days': 90,\n",
        "                    'StorageClass': 'GLACIER'  # Warm ‚Üí Cold\n",
        "                },\n",
        "                {\n",
        "                    'Days': 365,\n",
        "                    'StorageClass': 'DEEP_ARCHIVE'  # Cold ‚Üí Archive\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "s3_client.put_bucket_lifecycle_configuration(\n",
        "    Bucket='my-data-lake',\n",
        "    LifecycleConfiguration=lifecycle_policy\n",
        ")\n",
        "```\n",
        "\n",
        "**Ahorro de costes:** Mover datos antiguos a tiers fr√≠os puede reducir costes de almacenamiento en 70-90%.\n",
        "\n",
        "### Ejemplo real: Almacenamiento en Netflix\n",
        "\n",
        "**Arquitectura de almacenamiento:**\n",
        "\n",
        "```\n",
        "Transactional DBs (MySQL, Cassandra)\n",
        "    ‚Üì (CDC - Change Data Capture)\n",
        "S3 Data Lake (Bronze - Parquet)\n",
        "    ‚Üì (Spark jobs)\n",
        "S3 Data Lake (Silver/Gold - Iceberg tables)\n",
        "    ‚Üì (query engine)\n",
        "Redshift (agregados para dashboards)\n",
        "    ‚Üì\n",
        "Tableau/Looker (visualizaci√≥n)\n",
        "```\n",
        "\n",
        "**Vol√∫menes:**\n",
        "- **S3:** 100+ petabytes de datos hist√≥ricos\n",
        "- **Cassandra:** Decenas de TB de datos operacionales (perfiles de usuario, viewing history reciente)\n",
        "- **Redshift:** Varios TB de datos agregados para analytics\n",
        "\n",
        "**Decisiones de almacenamiento:**\n",
        "- **Bronze (raw logs):** Retenci√≥n 30 d√≠as en hot, despu√©s a Glacier\n",
        "- **Silver (processed):** Retenci√≥n 2 a√±os en S3 standard\n",
        "- **Gold (aggregates):** Indefinida, particionado por fecha para performance\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Etapa 4: Procesamiento y transformaci√≥n\n",
        "\n",
        "### Definici√≥n\n",
        "\n",
        "El **procesamiento** es donde los datos raw se transforman en datos √∫tiles para an√°lisis. Es la \"cocina\" donde se preparan los datos.\n",
        "\n",
        "### Tipos de procesamiento\n",
        "\n",
        "**1. Batch processing:**\n",
        "- Procesar grandes vol√∫menes en bloques\n",
        "- Schedule regular (diario, horario)\n",
        "- Tecnolog√≠as: Spark, dbt, SQL en warehouses\n",
        "\n",
        "**2. Stream processing:**\n",
        "- Procesar datos evento por evento en tiempo real\n",
        "- Latencia baja\n",
        "- Tecnolog√≠as: Flink, Kafka Streams, Spark Streaming\n",
        "\n",
        "**3. Micro-batch:**\n",
        "- H√≠brido: peque√±os batches frecuentes\n",
        "- Balance entre throughput y latencia\n",
        "\n",
        "### Operaciones comunes de transformaci√≥n\n",
        "\n",
        "**1. Limpieza (Cleaning):**\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Eliminar duplicados\n",
        "df = df.drop_duplicates(subset=['user_id', 'timestamp'])\n",
        "\n",
        "# Manejar valores faltantes\n",
        "df['age'].fillna(df['age'].median(), inplace=True)\n",
        "\n",
        "# Corregir tipos de datos\n",
        "df['order_date'] = pd.to_datetime(df['order_date'])\n",
        "\n",
        "# Eliminar outliers\n",
        "Q1 = df['revenue'].quantile(0.25)\n",
        "Q3 = df['revenue'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "df = df[(df['revenue'] >= Q1 - 1.5*IQR) & (df['revenue'] <= Q3 + 1.5*IQR)]\n",
        "```\n",
        "\n",
        "**2. Enriquecimiento (Enrichment):**\n",
        "- A√±adir informaci√≥n de otras fuentes\n",
        "- Geocoding (agregar pa√≠s a partir de IP)\n",
        "- Lookups (agregar nombre de producto a partir de ID)\n",
        "\n",
        "```sql\n",
        "-- Enriquecer transacciones con datos de cliente\n",
        "SELECT\n",
        "    t.transaction_id,\n",
        "    t.amount,\n",
        "    c.customer_name,\n",
        "    c.customer_segment,\n",
        "    c.lifetime_value\n",
        "FROM transactions t\n",
        "LEFT JOIN customers c ON t.customer_id = c.customer_id\n",
        "```\n",
        "\n",
        "**3. Agregaci√≥n:**\n",
        "- Resumir datos a nivel superior\n",
        "- De transacciones ‚Üí m√©tricas diarias\n",
        "- De eventos ‚Üí sesiones de usuario\n",
        "\n",
        "```sql\n",
        "-- Agregaci√≥n de transacciones a nivel diario\n",
        "SELECT\n",
        "    DATE(transaction_date) as date,\n",
        "    customer_id,\n",
        "    COUNT(*) as num_transactions,\n",
        "    SUM(amount) as total_spent,\n",
        "    AVG(amount) as avg_transaction_value\n",
        "FROM transactions\n",
        "GROUP BY DATE(transaction_date), customer_id\n",
        "```\n",
        "\n",
        "**4. Feature engineering (para ML):**\n",
        "- Crear variables predictivas\n",
        "\n",
        "```python\n",
        "# Features de tiempo\n",
        "df['hour'] = df['timestamp'].dt.hour\n",
        "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
        "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "# Features de comportamiento\n",
        "df['days_since_last_purchase'] = (\n",
        "    df.groupby('customer_id')['purchase_date']\n",
        "    .diff()\n",
        "    .dt.days\n",
        ")\n",
        "\n",
        "# Ratios\n",
        "df['revenue_per_item'] = df['total_revenue'] / df['num_items']\n",
        "\n",
        "# Encoding categ√≥rico\n",
        "df = pd.get_dummies(df, columns=['category'], prefix='cat')\n",
        "```\n",
        "\n",
        "### ETL vs ELT\n",
        "\n",
        "**ETL (Extract-Transform-Load):**\n",
        "```\n",
        "Source ‚Üí Transform (en servidor intermedio) ‚Üí Load (warehouse)\n",
        "```\n",
        "- Tradicional\n",
        "- Transformaci√≥n antes de cargar\n",
        "- M√°s lento pero datos llegan limpios\n",
        "\n",
        "**ELT (Extract-Load-Transform):**\n",
        "```\n",
        "Source ‚Üí Load (warehouse raw) ‚Üí Transform (dentro del warehouse)\n",
        "```\n",
        "- Moderno\n",
        "- Aprovechar poder de procesamiento del warehouse\n",
        "- M√°s r√°pido, m√°s flexible\n",
        "- **dbt es el est√°ndar para ELT**\n",
        "\n",
        "**Ejemplo dbt (ELT moderno):**\n",
        "\n",
        "```sql\n",
        "-- models/staging/stg_orders.sql\n",
        "with source as (\n",
        "    select * from {{ source('ecommerce', 'raw_orders') }}\n",
        "),\n",
        "\n",
        "cleaned as (\n",
        "    select\n",
        "        order_id,\n",
        "        customer_id,\n",
        "        order_date::date as order_date,\n",
        "        status,\n",
        "        case\n",
        "            when amount < 0 then 0\n",
        "            else amount\n",
        "        end as amount\n",
        "    from source\n",
        "    where order_date >= '2020-01-01'\n",
        ")\n",
        "\n",
        "select * from cleaned\n",
        "```\n",
        "\n",
        "### Data quality checks\n",
        "\n",
        "**Implementar validaciones en el procesamiento:**\n",
        "\n",
        "```python\n",
        "# Great Expectations para validaci√≥n de datos\n",
        "import great_expectations as ge\n",
        "\n",
        "df = ge.read_csv('orders.csv')\n",
        "\n",
        "# Expectations (reglas de calidad)\n",
        "df.expect_column_values_to_not_be_null('order_id')\n",
        "df.expect_column_values_to_be_unique('order_id')\n",
        "df.expect_column_values_to_be_between('amount', min_value=0, max_value=100000)\n",
        "df.expect_column_values_to_be_in_set('status', ['pending', 'completed', 'cancelled'])\n",
        "\n",
        "# Validar\n",
        "results = df.validate()\n",
        "\n",
        "if not results.success:\n",
        "    # Alertar si falla la calidad\n",
        "    send_alert(f\"Data quality check failed: {results}\")\n",
        "    raise Exception(\"Data quality issues detected\")\n",
        "```\n",
        "\n",
        "### Ejemplo real: Pipeline de procesamiento en Airbnb\n",
        "\n",
        "**Desaf√≠o:** Procesar datos de b√∫squedas, bookings y reviews para generar m√©tricas de negocio.\n",
        "\n",
        "**Pipeline:**\n",
        "\n",
        "```\n",
        "1. INGESTA (Kafka)\n",
        "   - Eventos de b√∫squeda (1M/hora)\n",
        "   - Eventos de booking (50K/d√≠a)\n",
        "   - Reviews (10K/d√≠a)\n",
        "         ‚Üì\n",
        "2. PROCESAMIENTO STREAM (Flink)\n",
        "   - Detecci√≥n de fraude en tiempo real\n",
        "   - Actualizaci√≥n de availability\n",
        "   - Alertas operacionales\n",
        "         ‚Üì\n",
        "3. BATCH PROCESSING (Spark + dbt)\n",
        "   - C√°lculo de m√©tricas de hosts (nightly)\n",
        "   - Agregaci√≥n de ocupaci√≥n por ciudad\n",
        "   - Features para modelo de pricing din√°mico\n",
        "         ‚Üì\n",
        "4. GOLD TABLES\n",
        "   - daily_bookings_by_city\n",
        "   - host_performance_metrics\n",
        "   - search_to_booking_funnel\n",
        "```\n",
        "\n",
        "**Frecuencia:**\n",
        "- Stream processing: Continuo (latencia `<5` segundos)\n",
        "- Batch processing: Diario a las 2 AM\n",
        "- Feature engineering para ML: Semanal\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Etapa 5: An√°lisis y uso\n",
        "\n",
        "### Definici√≥n\n",
        "\n",
        "El **an√°lisis** es donde finalmente extraemos valor de los datos, respondiendo preguntas de negocio, creando visualizaciones y entrenando modelos.\n",
        "\n",
        "### Tipos de an√°lisis\n",
        "\n",
        "**1. An√°lisis descriptivo (¬øQu√© pas√≥?):**\n",
        "- Dashboards de KPIs\n",
        "- Reportes de ventas, usuarios, engagement\n",
        "- **Herramientas:** Tableau, Power BI, Looker\n",
        "\n",
        "**2. An√°lisis diagn√≥stico (¬øPor qu√© pas√≥?):**\n",
        "- Root cause analysis\n",
        "- Segmentaci√≥n de clientes\n",
        "- An√°lisis de cohortes\n",
        "- **Herramientas:** SQL, Python (Pandas)\n",
        "\n",
        "**3. An√°lisis predictivo (¬øQu√© pasar√°?):**\n",
        "- Forecasting de demanda\n",
        "- Predicci√≥n de churn\n",
        "- Credit scoring\n",
        "- **Herramientas:** Scikit-learn, XGBoost, Prophet\n",
        "\n",
        "**4. An√°lisis prescriptivo (¬øQu√© deber√≠amos hacer?):**\n",
        "- Optimizaci√≥n de rutas\n",
        "- Pricing din√°mico\n",
        "- Recomendaciones personalizadas\n",
        "- **Herramientas:** OR-Tools, ML models, A/B testing\n",
        "\n",
        "### Democratizaci√≥n de datos\n",
        "\n",
        "**Self-service analytics:**\n",
        "\n",
        "Permitir que usuarios no t√©cnicos accedan y analicen datos sin depender de Data Engineers.\n",
        "\n",
        "**Componentes clave:**\n",
        "\n",
        "```\n",
        "Data Warehouse (Snowflake, BigQuery)\n",
        "        ‚Üì\n",
        "Semantic Layer (Looker, dbt Metrics)\n",
        "        ‚Üì\n",
        "BI Tools (Tableau, Metabase)\n",
        "        ‚Üì\n",
        "Business Users\n",
        "```\n",
        "\n",
        "**Beneficios:**\n",
        "- ‚úÖ Analistas responden sus propias preguntas\n",
        "- ‚úÖ Engineers se liberan para infraestructura\n",
        "- ‚úÖ Decisiones m√°s r√°pidas\n",
        "\n",
        "**Riesgos:**\n",
        "- ‚ùå Interpretaci√≥n incorrecta de m√©tricas\n",
        "- ‚ùå Queries ineficientes que ralentizan el warehouse\n",
        "- ‚ùå Proliferaci√≥n de dashboards redundantes\n",
        "\n",
        "**Soluci√≥n:** Semantic layer con definiciones centralizadas\n",
        "\n",
        "```yaml\n",
        "# dbt metric definition\n",
        "metrics:\n",
        "  - name: monthly_revenue\n",
        "    label: Monthly Revenue\n",
        "    model: ref('orders')\n",
        "    type: sum\n",
        "    sql: amount\n",
        "    timestamp: order_date\n",
        "    time_grains: [day, week, month]\n",
        "    dimensions:\n",
        "      - country\n",
        "      - product_category\n",
        "    filters:\n",
        "      - field: status\n",
        "        operator: '='\n",
        "        value: \"'completed'\"\n",
        "```\n",
        "\n",
        "**Resultado:** Todos usan la misma definici√≥n de \"Monthly Revenue\", reduciendo discrepancias.\n",
        "\n",
        "### Ejemplo de uso: Data-driven decision en Amazon\n",
        "\n",
        "**Pregunta de negocio:** ¬øDeber√≠amos ofrecer env√≠o gratuito en pedidos >30‚Ç¨?\n",
        "\n",
        "**An√°lisis realizado:**\n",
        "\n",
        "**1. An√°lisis descriptivo:**\n",
        "- 45% de pedidos est√°n entre 25-30‚Ç¨\n",
        "- AOV (Average Order Value) actual: 28‚Ç¨\n",
        "\n",
        "**2. An√°lisis diagn√≥stico:**\n",
        "- Clientes con pedidos 25-30‚Ç¨ tienen 2.3x m√°s probabilidad de abandonar carrito\n",
        "- Coste de env√≠o es la raz√≥n #1 de abandono\n",
        "\n",
        "**3. An√°lisis predictivo:**\n",
        "- Modelo predice: env√≠o gratuito >30‚Ç¨ aumentar√≠a AOV a 32‚Ç¨\n",
        "- 15% de clientes subir√≠an su pedido para alcanzar umbral\n",
        "\n",
        "**4. A/B Test:**\n",
        "- Grupo control: sin cambios\n",
        "- Grupo tratamiento: env√≠o gratis >30‚Ç¨\n",
        "- Duraci√≥n: 4 semanas\n",
        "\n",
        "**Resultados:**\n",
        "- AOV subi√≥ 14% (28‚Ç¨ ‚Üí 32‚Ç¨)\n",
        "- Tasa de conversi√≥n +8%\n",
        "- Revenue neto +12% (despu√©s de restar coste de env√≠os gratuitos)\n",
        "\n",
        "**Decisi√≥n:** Implementar env√≠o gratuito >30‚Ç¨ en toda la plataforma.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Etapa 6: Archivo (Archival)\n",
        "\n",
        "### Definici√≥n\n",
        "\n",
        "El **archivo** es el almacenamiento a largo plazo de datos hist√≥ricos que ya no se usan activamente pero deben preservarse por razones legales, de compliance o an√°lisis hist√≥rico eventual.\n",
        "\n",
        "### ¬øCu√°ndo archivar?\n",
        "\n",
        "**Se√±ales para archivar:**\n",
        "- Datos con `>1` a√±o de antig√ºedad\n",
        "- Acceso `<1` vez al mes\n",
        "- Requeridos por ley (retenci√≥n obligatoria)\n",
        "- Necesarios para auditor√≠as\n",
        "- Datos de entrenamiento de modelos hist√≥ricos\n",
        "\n",
        "### Estrategias de archivo\n",
        "\n",
        "**1. Cold storage en cloud:**\n",
        "\n",
        "```python\n",
        "# AWS S3 Glacier Deep Archive\n",
        "s3_client.copy_object(\n",
        "    CopySource={'Bucket': 'hot-data', 'Key': 'orders_2020.parquet'},\n",
        "    Bucket='archive',\n",
        "    Key='orders_2020.parquet',\n",
        "    StorageClass='DEEP_ARCHIVE'  # Muy bajo coste, retrieval en horas\n",
        ")\n",
        "```\n",
        "\n",
        "**Comparativa de costes AWS S3:**\n",
        "- Standard: $0.023/GB/mes\n",
        "- Glacier: $0.004/GB/mes (‚Üì83%)\n",
        "- Deep Archive: $0.00099/GB/mes (‚Üì96%)\n",
        "\n",
        "**2. Compresi√≥n:**\n",
        "\n",
        "```python\n",
        "# Comprimir antes de archivar\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow as pa\n",
        "\n",
        "# Leer datos\n",
        "df = pd.read_csv('large_file.csv')\n",
        "\n",
        "# Escribir con alta compresi√≥n\n",
        "pq.write_table(\n",
        "    pa.Table.from_pandas(df),\n",
        "    'large_file.parquet',\n",
        "    compression='zstd',  # Mejor compresi√≥n que gzip\n",
        "    compression_level=19  # M√°xima compresi√≥n\n",
        ")\n",
        "# Reducci√≥n t√≠pica: 70-90% del tama√±o original\n",
        "```\n",
        "\n",
        "**3. Particionamiento inteligente:**\n",
        "\n",
        "```\n",
        "archive/\n",
        "‚îú‚îÄ‚îÄ year=2020/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ month=01/\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet.gz\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ month=02/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "‚îú‚îÄ‚îÄ year=2021/\n",
        "‚îî‚îÄ‚îÄ year=2022/\n",
        "```\n",
        "\n",
        "**Beneficio:** Recuperar solo lo necesario sin descargar todo el archivo.\n",
        "\n",
        "### Data retention policies (pol√≠ticas de retenci√≥n)\n",
        "\n",
        "**Definir cu√°nto tiempo mantener cada tipo de dato:**\n",
        "\n",
        "| Tipo de dato | Retenci√≥n hot | Retenci√≥n archive | Motivo |\n",
        "|--------------|---------------|-------------------|--------|\n",
        "| Transacciones financieras | 3 meses | 7 a√±os | Ley (obligatorio) |\n",
        "| Logs de aplicaci√≥n | 30 d√≠as | 1 a√±o | Debugging |\n",
        "| Datos de usuarios | Mientras activo | N/A | GDPR (derecho al olvido) |\n",
        "| Datos de training ML | 1 a√±o | 5 a√±os | Reproducibilidad |\n",
        "| Emails | 90 d√≠as | 3 a√±os | Compliance |\n",
        "\n",
        "**Ejemplo de pol√≠tica automatizada:**\n",
        "\n",
        "```yaml\n",
        "# Data retention policy\n",
        "retention_policies:\n",
        "  - dataset: financial_transactions\n",
        "    hot_retention_days: 90\n",
        "    archive_retention_years: 7\n",
        "    deletion_allowed: false  # Nunca borrar\n",
        "    \n",
        "  - dataset: application_logs\n",
        "    hot_retention_days: 30\n",
        "    archive_retention_years: 1\n",
        "    deletion_allowed: true\n",
        "    \n",
        "  - dataset: user_clickstream\n",
        "    hot_retention_days: 60\n",
        "    archive_retention_years: 2\n",
        "    deletion_allowed: true\n",
        "```\n",
        "\n",
        "### Ejemplo real: Archivo en una entidad bancaria\n",
        "\n",
        "**Requisitos:**\n",
        "- Regulaci√≥n obliga retener transacciones 10 a√±os\n",
        "- Auditor√≠as requieren acceso a datos hist√≥ricos\n",
        "- Datos antiguos casi nunca se consultan\n",
        "\n",
        "**Soluci√≥n implementada:**\n",
        "\n",
        "```\n",
        "A√±o 0-1: PostgreSQL (hot - consultas diarias)\n",
        "    ‚Üì (automated migration)\n",
        "A√±o 1-3: Redshift (warm - consultas ocasionales)\n",
        "    ‚Üì\n",
        "A√±o 3-10: S3 Glacier (cold - solo auditor√≠as)\n",
        "    ‚Üì\n",
        "A√±o 10+: Eliminaci√≥n autom√°tica (cumplido el requisito legal)\n",
        "```\n",
        "\n",
        "**Coste:**\n",
        "- **Antes de archivo:** $50K/mes en almacenamiento hot\n",
        "- **Despu√©s:** $5K/mes (‚Üì90% en costes)\n",
        "\n",
        "**Tiempo de recuperaci√≥n:**\n",
        "- A√±o 1-3: `<1` minuto (Redshift)\n",
        "- A√±o 3-10: 12-48 horas (Glacier restore)\n",
        "- Aceptable porque accesos son raros\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Etapa 7: Eliminaci√≥n (Deletion)\n",
        "\n",
        "### Definici√≥n\n",
        "\n",
        "La **eliminaci√≥n** es el borrado permanente de datos que ya no son necesarios ni legalmente requeridos. Es la etapa final del ciclo de vida.\n",
        "\n",
        "### ¬øPor qu√© eliminar datos?\n",
        "\n",
        "**1. Compliance legal:**\n",
        "- GDPR: Derecho al olvido (Right to be forgotten)\n",
        "- CCPA (California): Derecho a eliminaci√≥n\n",
        "- Evitar retenci√≥n excesiva de datos personales\n",
        "\n",
        "**2. Reducci√≥n de riesgos:**\n",
        "- Menos datos = menor superficie de ataque\n",
        "- Reducir riesgo de data breaches\n",
        "- Datos que no tienes no pueden filtrarse\n",
        "\n",
        "**3. Reducci√≥n de costes:**\n",
        "- Almacenar datos innecesarios cuesta dinero\n",
        "- Menor complejidad operacional\n",
        "\n",
        "### Eliminaci√≥n l√≥gica vs f√≠sica\n",
        "\n",
        "**Eliminaci√≥n l√≥gica (soft delete):**\n",
        "```sql\n",
        "-- Marcar como eliminado, pero mantener el registro\n",
        "UPDATE users\n",
        "SET deleted_at = NOW(),\n",
        "    status = 'deleted'\n",
        "WHERE user_id = '12345';\n",
        "\n",
        "-- Las queries ignoran registros eliminados\n",
        "SELECT * FROM users WHERE deleted_at IS NULL;\n",
        "```\n",
        "\n",
        "**Ventajas:**\n",
        "- ‚úÖ Recuperable si fue error\n",
        "- ‚úÖ Mantiene integridad referencial\n",
        "- ‚úÖ Auditable\n",
        "\n",
        "**Desventajas:**\n",
        "- ‚ùå Datos siguen ocupando espacio\n",
        "- ‚ùå No cumple GDPR estricto\n",
        "\n",
        "**Eliminaci√≥n f√≠sica (hard delete):**\n",
        "```sql\n",
        "-- Borrado permanente\n",
        "DELETE FROM users WHERE user_id = '12345';\n",
        "-- Eliminar todas las referencias en cascada\n",
        "DELETE FROM orders WHERE user_id = '12345';\n",
        "DELETE FROM reviews WHERE user_id = '12345';\n",
        "```\n",
        "\n",
        "**Ventajas:**\n",
        "- ‚úÖ Datos verdaderamente eliminados\n",
        "- ‚úÖ Cumple GDPR\n",
        "- ‚úÖ Libera almacenamiento\n",
        "\n",
        "**Desventajas:**\n",
        "- ‚ùå Irreversible\n",
        "- ‚ùå Puede romper integridad si no se hace bien\n",
        "- ‚ùå Complejo en sistemas distribuidos\n",
        "\n",
        "### Desaf√≠os de eliminaci√≥n\n",
        "\n",
        "**1. Datos distribuidos:**\n",
        "\n",
        "Datos de un usuario pueden estar en 20+ tablas/sistemas:\n",
        "```\n",
        "User data ubicaciones:\n",
        "‚îú‚îÄ PostgreSQL (users table)\n",
        "‚îú‚îÄ MongoDB (preferences)\n",
        "‚îú‚îÄ Elasticsearch (search index)\n",
        "‚îú‚îÄ S3 (uploaded files)\n",
        "‚îú‚îÄ Redshift (analytics warehouse)\n",
        "‚îú‚îÄ Redis (cache)\n",
        "‚îú‚îÄ Kafka (event logs)\n",
        "‚îî‚îÄ Backups (m√∫ltiples ubicaciones)\n",
        "```\n",
        "\n",
        "**Soluci√≥n:** Data mapping completo + proceso orquestado\n",
        "\n",
        "```python\n",
        "# Orquestaci√≥n de eliminaci√≥n GDPR\n",
        "class GDPRDeletionService:\n",
        "    def delete_user_data(self, user_id):\n",
        "        deleted_locations = []\n",
        "        \n",
        "        # 1. Eliminar de sistemas operacionales\n",
        "        postgres.delete(f\"DELETE FROM users WHERE user_id = '{user_id}'\")\n",
        "        deleted_locations.append('postgres')\n",
        "        \n",
        "        # 2. Eliminar de analytics\n",
        "        redshift.delete(f\"DELETE FROM dim_users WHERE user_id = '{user_id}'\")\n",
        "        deleted_locations.append('redshift')\n",
        "        \n",
        "        # 3. Eliminar archivos en S3\n",
        "        s3.delete_objects_by_prefix(f\"user_data/{user_id}/\")\n",
        "        deleted_locations.append('s3')\n",
        "        \n",
        "        # 4. Limpiar cach√©\n",
        "        redis.delete(f\"user:{user_id}\")\n",
        "        deleted_locations.append('redis')\n",
        "        \n",
        "        # 5. Eliminar de √≠ndices de b√∫squeda\n",
        "        elasticsearch.delete_by_query(f\"user_id:{user_id}\")\n",
        "        deleted_locations.append('elasticsearch')\n",
        "        \n",
        "        # 6. Registrar en audit log\n",
        "        audit_log.record({\n",
        "            'action': 'gdpr_deletion',\n",
        "            'user_id': user_id,\n",
        "            'timestamp': datetime.now(),\n",
        "            'locations': deleted_locations\n",
        "        })\n",
        "        \n",
        "        return {'success': True, 'locations_deleted': deleted_locations}\n",
        "```\n",
        "\n",
        "**2. Backups:**\n",
        "\n",
        "Desaf√≠o: Datos eliminados pueden seguir en backups antiguos.\n",
        "\n",
        "**Soluci√≥n:**\n",
        "- Encriptar backups con claves que se pueden destruir\n",
        "- Implementar \"crypto-shredding\" (destruir clave = datos ilegibles)\n",
        "- Retenci√≥n limitada de backups (30-90 d√≠as)\n",
        "\n",
        "**3. Anonymizaci√≥n como alternativa:**\n",
        "\n",
        "En lugar de eliminar, anonimizar para analytics:\n",
        "\n",
        "```python\n",
        "def anonymize_user(user_id):\n",
        "    # Reemplazar PII con valores sint√©ticos pero estad√≠sticamente similares\n",
        "    user = get_user(user_id)\n",
        "    \n",
        "    anonymized = {\n",
        "        'user_id': hash(user_id),  # Hash irreversible\n",
        "        'age': round_to_range(user.age),  # 25 ‚Üí \"20-30\"\n",
        "        'country': user.country,  # Mantener (no identificable)\n",
        "        'email': None,  # Eliminar completamente\n",
        "        'name': None,\n",
        "        'purchase_history': [\n",
        "            {'amount': p.amount, 'category': p.category}\n",
        "            # Mantener montos y categor√≠as, eliminar detalles espec√≠ficos\n",
        "            for p in user.purchases\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    return anonymized\n",
        "```\n",
        "\n",
        "**Beneficio:** Mantener utilidad anal√≠tica sin identificar individuos.\n",
        "\n",
        "### Pol√≠ticas de eliminaci√≥n automatizada\n",
        "\n",
        "```yaml\n",
        "# Automated deletion policy\n",
        "deletion_policies:\n",
        "  - trigger: user_account_deletion_request\n",
        "    retention_period: 30_days  # Grace period\n",
        "    cascade_delete:\n",
        "      - users\n",
        "      - orders\n",
        "      - reviews\n",
        "      - uploads\n",
        "    notify:\n",
        "      - user_email\n",
        "      - compliance_team\n",
        "    \n",
        "  - trigger: data_retention_exceeded\n",
        "    datasets:\n",
        "      - application_logs: 90_days\n",
        "      - marketing_analytics: 2_years\n",
        "    schedule: daily_at_2am\n",
        "```\n",
        "\n",
        "### Ejemplo real: GDPR deletion en Facebook/Meta\n",
        "\n",
        "**Complejidad:**\n",
        "- Datos de un usuario en 100+ sistemas internos\n",
        "- Millones de interacciones con otros usuarios (likes, comments, tags)\n",
        "- Backups distribuidos globalmente\n",
        "\n",
        "**Soluci√≥n implementada:**\n",
        "1. **User request:** Usuario solicita borrado desde configuraci√≥n\n",
        "2. **Grace period:** 30 d√≠as para arrepentirse (cuenta suspendida pero datos intactos)\n",
        "3. **Cascading deletion:** Proceso de 90 d√≠as\n",
        "   - D√≠a 1-30: Eliminar de sistemas de cara al usuario\n",
        "   - D√≠a 31-60: Eliminar de sistemas de analytics\n",
        "   - D√≠a 61-90: Eliminar de backups y logs\n",
        "4. **Anonymization:** Algunos datos agregados se anonimizan en lugar de eliminar\n",
        "5. **Audit trail:** Todo el proceso registrado para auditor√≠as\n",
        "\n",
        "**Desaf√≠o continuo:** Backups pre-GDPR a√∫n contienen datos antiguos hasta que expiran.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Gobernanza del ciclo de vida\n",
        "\n",
        "### Data governance framework\n",
        "\n",
        "La **gobernanza** asegura que cada etapa del ciclo de vida cumpla est√°ndares de calidad, seguridad y compliance.\n",
        "\n",
        "### Componentes de gobernanza\n",
        "\n",
        "**1. Data lineage (linaje de datos):**\n",
        "\n",
        "Rastrear el origen y transformaciones de cada dato.\n",
        "\n",
        "```\n",
        "Source: Salesforce API\n",
        "    ‚Üì (Ingestion: Fivetran)\n",
        "Bronze: s3://raw/salesforce/opportunities/\n",
        "    ‚Üì (Transformation: dbt)\n",
        "Silver: s3://cleaned/opportunities/\n",
        "    ‚Üì (Aggregation: Spark)\n",
        "Gold: redshift.gold.daily_sales\n",
        "    ‚Üì (Visualization: Tableau)\n",
        "Dashboard: \"Sales Performance\"\n",
        "```\n",
        "\n",
        "**Herramientas:** OpenLineage, DataHub, Apache Atlas\n",
        "\n",
        "**Beneficio:**\n",
        "- Debugging: \"¬øPor qu√© este n√∫mero cambi√≥?\"\n",
        "- Impact analysis: \"Si modifico esta tabla, ¬øqu√© se rompe?\"\n",
        "- Compliance: \"Demostrar origen de datos en auditor√≠a\"\n",
        "\n",
        "**2. Data quality monitoring:**\n",
        "\n",
        "Monitorizar calidad continuamente:\n",
        "\n",
        "```python\n",
        "# Great Expectations checkpoint\n",
        "checkpoint_config = {\n",
        "    'name': 'daily_orders_quality_check',\n",
        "    'expectations': [\n",
        "        {\n",
        "            'expectation_type': 'expect_column_values_to_not_be_null',\n",
        "            'column': 'order_id'\n",
        "        },\n",
        "        {\n",
        "            'expectation_type': 'expect_column_values_to_be_between',\n",
        "            'column': 'amount',\n",
        "            'min_value': 0,\n",
        "            'max_value': 100000\n",
        "        },\n",
        "        {\n",
        "            'expectation_type': 'expect_table_row_count_to_be_between',\n",
        "            'min_value': 10000,\n",
        "            'max_value': 100000\n",
        "        }\n",
        "    ],\n",
        "    'actions': [\n",
        "        {\n",
        "            'name': 'send_slack_alert_on_failure',\n",
        "            'webhook': 'https://hooks.slack.com/...'\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "**3. Data catalog:**\n",
        "\n",
        "Inventario centralizado de todos los datos:\n",
        "\n",
        "```yaml\n",
        "# Data catalog entry\n",
        "dataset: customers\n",
        "description: \"Customer master data\"\n",
        "owner: data-platform-team\n",
        "pii_data: true\n",
        "retention_policy: 3_years\n",
        "update_frequency: daily\n",
        "schema:\n",
        "  - name: customer_id\n",
        "    type: string\n",
        "    description: \"Unique customer identifier\"\n",
        "    pii: false\n",
        "  - name: email\n",
        "    type: string\n",
        "    description: \"Customer email\"\n",
        "    pii: true\n",
        "  - name: lifetime_value\n",
        "    type: decimal\n",
        "    description: \"Total revenue from customer\"\n",
        "    pii: false\n",
        "tags:\n",
        "  - customer-data\n",
        "  - pii\n",
        "  - tier-1\n",
        "```\n",
        "\n",
        "**Herramientas:** Alation, Collibra, DataHub\n",
        "\n",
        "**4. Access control:**\n",
        "\n",
        "Qui√©n puede acceder a qu√© datos:\n",
        "\n",
        "```sql\n",
        "-- Row-level security en Snowflake\n",
        "CREATE ROW ACCESS POLICY customer_data_policy AS (customer_country STRING)\n",
        "RETURNS BOOLEAN ->\n",
        "  CASE\n",
        "    WHEN CURRENT_ROLE() IN ('ADMIN', 'COMPLIANCE') THEN TRUE\n",
        "    WHEN CURRENT_ROLE() = 'EU_ANALYST' AND customer_country IN ('ES', 'FR', 'DE') THEN TRUE\n",
        "    WHEN CURRENT_ROLE() = 'US_ANALYST' AND customer_country = 'US' THEN TRUE\n",
        "    ELSE FALSE\n",
        "  END;\n",
        "\n",
        "ALTER TABLE customers\n",
        "ADD ROW ACCESS POLICY customer_data_policy ON (country);\n",
        "```\n",
        "\n",
        "**5. Audit logging:**\n",
        "\n",
        "Registrar todos los accesos a datos sensibles:\n",
        "\n",
        "```python\n",
        "# Audit log entry\n",
        "{\n",
        "    'timestamp': '2025-10-06T14:23:15Z',\n",
        "    'user': 'analyst@company.com',\n",
        "    'action': 'SELECT',\n",
        "    'table': 'customers',\n",
        "    'columns': ['email', 'name', 'lifetime_value'],\n",
        "    'rows_accessed': 15023,\n",
        "    'query': 'SELECT email, name, lifetime_value FROM customers WHERE country = ES',\n",
        "    'ip_address': '192.168.1.100',\n",
        "    'approved': true\n",
        "}\n",
        "```\n",
        "\n",
        "### Roles y responsabilidades\n",
        "\n",
        "| Rol | Responsabilidad en gobernanza |\n",
        "|-----|------------------------------|\n",
        "| **Data Steward** | Define reglas de calidad y acceso |\n",
        "| **Data Engineer** | Implementa controles t√©cnicos |\n",
        "| **Data Analyst** | Reporta problemas de calidad |\n",
        "| **Legal/Compliance** | Define pol√≠ticas de retenci√≥n y privacidad |\n",
        "| **Security** | Implementa controles de acceso |\n",
        "| **Data Architect** | Dise√±a arquitectura de gobernanza |\n",
        "\n",
        "---\n",
        "\n",
        "## 11. M√©tricas del ciclo de vida\n",
        "\n",
        "### KPIs para monitorizar\n",
        "\n",
        "**1. Data freshness (frescura):**\n",
        "- ¬øCu√°n actualizados est√°n los datos?\n",
        "- M√©trica: `current_time - last_update_time`\n",
        "- Target: <24h para reportes, <1h para operacional\n",
        "\n",
        "**2. Data quality score:**\n",
        "- % de registros que pasan validaciones\n",
        "- Target: >99%\n",
        "\n",
        "**3. Storage costs:**\n",
        "- Coste por TB almacenado\n",
        "- Tendencia a lo largo del tiempo\n",
        "- Target: Reducci√≥n 10% anual por optimizaciones\n",
        "\n",
        "**4. Time to insights:**\n",
        "- Desde generaci√≥n hasta disponible para an√°lisis\n",
        "- Target: <24h para batch, <5min para streaming\n",
        "\n",
        "**5. Data coverage:**\n",
        "- % de datos que tienen metadata y lineage\n",
        "- Target: >90%\n",
        "\n",
        "**6. Compliance rate:**\n",
        "- % de datasets cumpliendo pol√≠ticas de retenci√≥n\n",
        "- Target: 100%\n",
        "\n",
        "### Dashboard de monitorizaci√≥n\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Data Platform Health Dashboard                         ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  Data Freshness:           ‚úÖ 98% < 24h                 ‚îÇ\n",
        "‚îÇ  Quality Score:            ‚ö†Ô∏è  96.5% (target: 99%)       ‚îÇ\n",
        "‚îÇ  Storage Growth:           üìà +15% MoM                  ‚îÇ\n",
        "‚îÇ  Failed Pipelines:         ‚ùå 3 in last 24h             ‚îÇ\n",
        "‚îÇ  GDPR Deletion Requests:   ‚è±Ô∏è  12 pending               ‚îÇ\n",
        "‚îÇ  Data Catalog Coverage:    ‚úÖ 94%                       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Conceptos clave\n",
        "\n",
        "- **Ciclo de vida:** Datos pasan por 7 etapas desde generaci√≥n hasta eliminaci√≥n\n",
        "- **Generaci√≥n:** Momento cr√≠tico para calidad (validar en origen)\n",
        "- **Ingesta:** Batch vs streaming seg√∫n latencia requerida\n",
        "- **Almacenamiento:** Medallion architecture (Bronze ‚Üí Silver ‚Üí Gold)\n",
        "- **Hot/Warm/Cold:** Estratificar datos seg√∫n frecuencia de acceso\n",
        "- **Procesamiento:** ETL tradicional vs ELT moderno (dbt)\n",
        "- **Data quality:** Validaciones automatizadas en cada etapa\n",
        "- **Archivo:** Reducir costes moviendo datos antiguos a cold storage\n",
        "- **Eliminaci√≥n:** GDPR exige capacidad de borrado completo\n",
        "- **Data lineage:** Rastrear origen y transformaciones para debugging y compliance\n",
        "- **Gobernanza:** Pol√≠ticas, controles y monitorizaci√≥n a lo largo del ciclo\n",
        "- **Idempotencia:** Procesos repetibles con mismo resultado\n",
        "- **Retenci√≥n:** Pol√≠ticas claras de cu√°nto tiempo mantener cada tipo de dato\n",
        "\n",
        "---\n",
        "\n",
        "## Resumen\n",
        "\n",
        "El ciclo de vida de los datos en la organizaci√≥n es un flujo continuo que abarca siete etapas interdependientes: generaci√≥n, recolecci√≥n, almacenamiento, procesamiento, an√°lisis/uso, archivo y eliminaci√≥n. Comprender este ciclo completo es fundamental para dise√±ar arquitecturas de datos eficientes, garantizar calidad, cumplir regulaciones y maximizar el valor extra√≠do.\n",
        "\n",
        "Los datos nacen en la **generaci√≥n**, donde la validaci√≥n temprana previene problemas posteriores. Son capturados mediante **ingesta** (batch o streaming seg√∫n latencia requerida) y **almacenados** en arquitecturas modernas como data lakehouses con estrategia medallion (Bronze-Silver-Gold). El **procesamiento** transforma datos raw en informaci√≥n √∫til, siguiendo cada vez m√°s el paradigma ELT con herramientas como dbt. El **an√°lisis** extrae valor mediante descriptive, predictive y prescriptive analytics. Los datos hist√≥ricos se mueven a **archivo** (cold storage) reduciendo costes 70-90%, y finalmente se **eliminan** cuando ya no son necesarios o requeridos legalmente.\n",
        "\n",
        "La **gobernanza** atraviesa todas las etapas: data lineage rastrea transformaciones, quality monitoring detecta anomal√≠as, data catalogs documentan assets, y access controls protegen informaci√≥n sensible. Pol√≠ticas de retenci√≥n claras, implementadas t√©cnicamente con lifecycle policies automatizadas, equilibran valor de negocio con compliance (especialmente GDPR).\n",
        "\n",
        "El √©xito no est√° en cada etapa aislada, sino en la orquestaci√≥n fluida del ciclo completo: datos que fluyen eficientemente desde su origen hasta generar insights accionables, archiv√°ndose cuando pierden relevancia y elimin√°ndose cuando es necesario, todo bajo un framework robusto de gobernanza que asegura calidad, seguridad y cumplimiento legal.\n",
        "\n",
        "---\n",
        "\n",
        "## Referencias\n",
        "\n",
        "### V√≠deos\n",
        "- [Data Lifecycle Management Explained](https://www.youtube.com/results?search_query=data+lifecycle+management)\n",
        "- [What is Data Lineage?](https://www.youtube.com/results?search_query=data+lineage+explained)\n",
        "- [ETL vs ELT: Which is Better?](https://www.youtube.com/results?search_query=etl+vs+elt)\n",
        "- [GDPR and Data Deletion Requirements](https://www.youtube.com/results?search_query=gdpr+data+deletion)\n",
        "- [Building a Data Lakehouse](https://www.youtube.com/results?search_query=data+lakehouse+architecture)\n",
        "\n",
        "### Lecturas\n",
        "- [The Medallion Architecture - Databricks](https://www.databricks.com/glossary/medallion-architecture)\n",
        "- [Data Lifecycle Management Best Practices - AWS](https://aws.amazon.com/blogs/storage/best-practices-for-data-lifecycle-management-on-amazon-s3/)\n",
        "- [Modern Data Stack Explained - dbt](https://www.getdbt.com/blog/future-of-the-modern-data-stack/)\n",
        "- [GDPR Right to Erasure - Official Guide](https://gdpr-info.eu/art-17-gdpr/)\n",
        "- [Data Quality in the Modern Data Stack](https://www.montecarlodata.com/blog-what-is-data-quality/)\n",
        "\n",
        "### Herramientas\n",
        "- [Great Expectations - Data Quality](https://greatexpectations.io/)\n",
        "- [dbt - Data Transformation](https://www.getdbt.com/)\n",
        "- [Apache Airflow - Orchestration](https://airflow.apache.org/)\n",
        "- [DataHub - Data Catalog & Lineage](https://datahubproject.io/)\n",
        "- [Delta Lake - Lakehouse Storage](https://delta.io/)\n",
        "- [OpenLineage - Lineage Standard](https://openlineage.io/)\n",
        "\n",
        "### Gu√≠as pr√°cticas\n",
        "- [AWS S3 Lifecycle Configuration](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html)\n",
        "- [Implementing GDPR Data Deletion](https://www.atlassian.com/blog/confluence/gdpr-data-deletion-guide)\n",
        "- [Building Data Pipelines with Airflow](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html)\n",
        "- [Data Governance Framework - DAMA](https://www.dama.org/cpages/body-of-knowledge)"
      ],
      "metadata": {
        "id": "GqNRXaggakMa"
      }
    }
  ]
}